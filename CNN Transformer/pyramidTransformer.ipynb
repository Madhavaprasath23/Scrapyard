{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to do with the transformer module\n",
    "#### 1) Block for the achitecture\n",
    "####     2) Transformer block finilized with interactable modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhava\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import torchmetrics\n",
    "import datetime\n",
    "from functools import partial\n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _cfg\n",
    "from ConvFFN import CONVFFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DropPath(drop_prob=0.300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DropPath(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    " checkpoint_wrapper,\n",
    " CheckpointImpl )\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    "    BackwardPrefetch,\n",
    "    ShardingStrategy,\n",
    "    FullStateDictConfig,\n",
    "    StateDictType,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    transformer_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "from CNNbackbone import Make_CNN_BACK_BONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,image_size=224,patch_size=7,stride=4,in_chans=3,embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size=(image_size,image_size)\n",
    "        self.patch_size=(patch_size,patch_size)\n",
    "\n",
    "        assert max(self.patch_size)>stride, \"larger than stride\"\n",
    "\n",
    "        self.image_size=(image_size,image_size)\n",
    "        self.patch_size=(patch_size,patch_size)\n",
    "\n",
    "        self.H,self.W=self.image_size[0]//stride,self.image_size[1]//stride\n",
    "\n",
    "        self.proj=nn.Conv2d(in_chans,embed_dim,kernel_size=self.patch_size,stride=stride,\n",
    "                            padding=(self.patch_size[0]//2,self.patch_size[1]//2))\n",
    "        self.norm=nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        print(x.shape,\"Projection\")\n",
    "        _, _, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, H, W\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverlapPatchEmbedding(\n",
    "                image_size=self.image_size if i==0 else self.image_size//(2**(i + 1)),\n",
    "                patch_size= self.patch_size if i==0 else 3,\n",
    "                stride= 4 if i==0 else 2,\n",
    "                in_chans=3 if i==0 else self.embeded_dimesion[i-1],\n",
    "                embed_dim=self.embeded_dimesion[i]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBlock_attention(nn.Module):\n",
    "    def __init__(self,dim,num_heads=4,qkv_bias=False,qk_scale=None,attn_drop=0.,proj_drop=0.):\n",
    "        self.dim=dim\n",
    "        self.num_heads=4\n",
    "        \n",
    "        self.q=nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        self.kv=nn.Linear(dim,dim*2,bias=qkv_bias)\n",
    "        self.qscale=self.num_heads**-0.5\n",
    "        self.attn_drop=nn.Dropout(attn_drop)\n",
    "        self.proj_drop=nn.Dropout(proj_drop)\n",
    "        self.norm=nn.LayerNorm(dim)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self,k,v):\n",
    "        stacked=torch.stack([k,v],dim=0) # b d n c -> qk b d n c\n",
    "        x=rearrange(stacked,'qk b d n c -> b n (d c qk)')\n",
    "\n",
    "        q= self.q(x)\n",
    "        \n",
    "        kv=self.kv(x)\n",
    "\n",
    "        q=rearrange(q,'b n (d c) -> b d n c',d=self.dim)\n",
    "\n",
    "        kv=rearrange(kv,'b n (qk d c) -> qk b d n c',qk=2,d=self.num_heads)\n",
    "        \n",
    "        #seprate k and v\n",
    "\n",
    "        k,v=kv[0],kv[1]\n",
    "        #attention mechanism and send only attention map \n",
    "\n",
    "        attn=einsum('b i j l, b i k m -> b i j k',q,k)*self.qscale\n",
    "\n",
    "        attn=attn.softmax(dim=-1)\n",
    "        attn=self.attn_drop(attn)\n",
    "        attn= einsum('b d l n, b d n j-> b d l j',attn,v)\n",
    "        attn=rearrange(attn,'b d n c -> b n (c d)')\n",
    "\n",
    "        final=x+attn\n",
    "        final=self.norm(final)\n",
    "        final=rearrange(final,'b n (qk d c) -> qk b d n c',qk=2,d=self.num_heads)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim,num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1, linear=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.dim=dim\n",
    "\n",
    "        self.head_dim= dim// num_heads\n",
    "        self.qk_scale = qk_scale or self.head_dim ** -0.5\n",
    "\n",
    "        self.sr_ratio = sr_ratio\n",
    "\n",
    "        self.linear = linear\n",
    "\n",
    "        self.q= nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv= nn.Linear(dim, dim*2, bias=qkv_bias)\n",
    "        self.proj= nn.Linear(dim, dim)  \n",
    "        self.attn_drop= nn.Dropout(attn_drop)\n",
    "        self.proj_drop= nn.Dropout(proj_drop)\n",
    "        self.k_learnable=nn.Parameter(torch.randn(dim), requires_grad=True)\n",
    "        self.v_learnable=nn.Parameter(torch.randn(dim), requires_grad=True)\n",
    "        if not linear:\n",
    "            if self.sr_ratio > 1:\n",
    "                self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "                self.norm = nn.LayerNorm(dim)\n",
    "        else:\n",
    "            self.adaptive_pooling=nn.AdaptiveAvgPool2d(7)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "            self.act=nn.GELU()\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=1, stride=1)\n",
    "    \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self,x,H,W):\n",
    "        B,N,C=x.shape\n",
    "\n",
    "        qv=self.q(x)\n",
    "        qv=rearrange(qv,'b n (d c) -> b d n c',d=self.num_heads)\n",
    "\n",
    "        if not self.linear:\n",
    "            if self.sr_ratio >1:\n",
    "                x_=rearrange(x,'B (H W) C -> B C H W',H=H,W=W)\n",
    "                x_=rearrange(self.sr(x_),'B C H W -> B (H W) C')\n",
    "                x_=self.norm(x_)\n",
    "                kv=self.kv(x_)\n",
    "                kv=rearrange(kv,'B N (d c qk) -> qk B d N c',d=self.num_heads,c=C//self.num_heads)\n",
    "            else:\n",
    "                kv=self.kv(x)\n",
    "                kv=rearrange(kv,'b n (d c qk) -> qk b d n c',d=self.num_heads,c=C//self.num_heads)\n",
    "            \n",
    "        else:\n",
    "            x_=rearrange(x,'b (H W) C -> b C H W', H=H,W=W)\n",
    "            x_=self.sr(self.adaptive_pooling(x_))\n",
    "            x_=rearrange(x_,'b c h w -> b (h w) c')\n",
    "            x_=self.norm(x_)\n",
    "            x_=self.act(x_)\n",
    "            kv=rearrange(kv,'b n (d c qk) -> qk b d n c',d=self.num_heads,c=C//self.num_heads)\n",
    "\n",
    "        k,v=kv[0],kv[1]\n",
    "\n",
    "\n",
    "        l_k=repeat(self.k_learnable,'h1 -> b h1',b=B) # Batch h1\n",
    "        \n",
    "\n",
    "        l_v=repeat(self.v_learnable,'h1  -> b h1 ',b=B) # Batch h1 \n",
    "\n",
    "\n",
    "        l_v=rearrange(l_v,\"b (a d)-> b a d\",a=self.num_heads)\n",
    "        l_k=rearrange(l_k,\"b (a d)-> b a d\",a=self.num_heads)\n",
    "\n",
    "        k=rearrange(k,'b head n c -> b n head c')\n",
    "        v=rearrange(v,'b head n c -> b n head c')\n",
    "\n",
    "        k=einsum('b n h c, b p q -> b p n q',k,l_k)\n",
    "        v=einsum('b n h c, b p q -> b p n q',v,l_v)\n",
    "\n",
    "\n",
    "        attn=einsum('b i j l, b i k m -> b i j k',qv,k)*self.qk_scale\n",
    "\n",
    "        \n",
    "\n",
    "        attn=attn.softmax(dim=-1)\n",
    "\n",
    "        attn=self.attn_drop(attn)\n",
    "\n",
    "\n",
    "        attn= einsum('b d l n, b d n j-> b d l j',attn,v)\n",
    "        attn=rearrange(attn,'b d n c -> b n (c d)')\n",
    "        \n",
    "        \n",
    "        x= self.proj(attn)\n",
    "        x=self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,dim,heads,qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1, linear=False,mlp_ratio=4.0,norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "        self.heads=heads\n",
    "        self.qkv_bias=qkv_bias\n",
    "        self.qk_scale=qk_scale\n",
    "        self.attn_drop=DropPath(attn_drop) if attn_drop>0 else nn.Identity()\n",
    "        self.proj_drop=DropPath(proj_drop) if proj_drop>0 else nn.Identity()\n",
    "        self.sr_ratio=sr_ratio\n",
    "        self.linear=linear\n",
    "        \n",
    "\n",
    "        self.norm1=norm_layer(self.dim)\n",
    "\n",
    "        self.attn=Attention(dim=self.dim,num_heads=self.heads,qkv_bias=self.qkv_bias,qk_scale=self.qk_scale,\n",
    "                            attn_drop=attn_drop,proj_drop=proj_drop,sr_ratio=self.sr_ratio,linear=self.linear)\n",
    "        \n",
    "        self.norm2=norm_layer(self.dim)\n",
    "\n",
    "        self.mlp_dim= int(self.dim * mlp_ratio)\n",
    "\n",
    "        self.conv_mlp= CONVFFN(in_channels=self.dim,hidden_dimension=self.mlp_dim)\n",
    "\n",
    "    \n",
    "    def forward(self,x,H,W):\n",
    "\n",
    "        x=x + self.attn_drop(self.attn(self.norm1(x),H,W))\n",
    "        x=x + self.proj_drop(self.conv_mlp(self.norm2(x),H,W))\n",
    "        print(x.shape,\"Layer1 norm\")\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePatchAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self,image_size=224,patch_size=7,embeded_dimesion=[64,128,320,512],depth=[3,6,40,3],\n",
    "                 num_heads=[1,2,5,8], sr_ratio=[8,4,2,1],norm_layer=nn.LayerNorm,qkv_bias=True,qk_scale=None,attn_drop=0.1,proj_drop=0.1,\n",
    "                 stages=4,mlp_ratio=[8.0,8.0,4.0,4.0],linear=True):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size=image_size\n",
    "        self.patch_size=patch_size\n",
    "        self.embeded_dimesion=embeded_dimesion\n",
    "        self.depth=depth\n",
    "        self.num_heads=num_heads\n",
    "        self.norm_layer=norm_layer\n",
    "        self.qkv_bias=qkv_bias\n",
    "        self.qk_scale=qk_scale\n",
    "        self.attn_drop=attn_drop\n",
    "        self.proj_drop=proj_drop\n",
    "\n",
    "        self.stages=stages\n",
    "        self.mlp_ratio=mlp_ratio\n",
    "\n",
    "        dpr=[x.item() for x in torch.linspace(0,self.proj_drop,sum(self.depth))]\n",
    "        cur=0\n",
    "        print(dpr)\n",
    "        \n",
    "        for i in range(self.stages):\n",
    "\n",
    "            \n",
    "\n",
    "            patch_embedding=OverlapPatchEmbedding(\n",
    "                image_size=self.image_size if i==0 else self.image_size//(2**(i + 1)),\n",
    "                patch_size= 7 if i==0 else 3,\n",
    "                stride= 4 if i==0 else 2,\n",
    "                in_chans=3 if i==0 else self.embeded_dimesion[i-1],\n",
    "                embed_dim=self.embeded_dimesion[i]\n",
    "            )\n",
    "\n",
    "            if i==0:\n",
    "                #learnable_weights=nn.Parameter(\n",
    "                 #   torch.randn(2,3196,64),\n",
    "                  #  requires_grad=True\n",
    "                #) verision 2\n",
    "                pass\n",
    "            attn=nn.ModuleList(\n",
    "                [\n",
    "                    Block(\n",
    "                        dim=embeded_dimesion[i],\n",
    "                        qkv_bias=True,\n",
    "                        qk_scale=None,\n",
    "                        attn_drop=attn_drop,\n",
    "                        proj_drop=dpr[cur+j],\n",
    "                        sr_ratio=sr_ratio[i],\n",
    "                        linear=linear,\n",
    "                        mlp_ratio=self.mlp_ratio[i],\n",
    "                        heads=self.num_heads[i]\n",
    "                        \n",
    "                    ) for j in range(self.depth[i])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            norm=norm_layer(embeded_dimesion[i])\n",
    "\n",
    "\n",
    "\n",
    "            cur+=self.depth[i]\n",
    "\n",
    "            setattr(self,f'patch_embed{i +1}',patch_embedding)\n",
    "            \"\"\"if i==0:\n",
    "                setattr(self,f'learnable_weights_{i +1}',learnable_weights) version 2\"\"\"\n",
    "\n",
    "            setattr(self,f'block{i +1}',attn)\n",
    "            setattr(self,f'norm{i +1}',norm)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "\n",
    "    def forward_features(self,x):\n",
    "        B=x.shape[0]\n",
    "        c=0\n",
    "\n",
    "        for i in range(self.stages):\n",
    "            patch_embedding=getattr(self,f'patch_embed{i +1}')\n",
    "            Block=getattr(self,f'block{i +1}')\n",
    "            norm=getattr(self,f'norm{i +1}')\n",
    "\n",
    "            x,H,W=patch_embedding(x)\n",
    "            for blk in Block:\n",
    "                x=blk(x,H,W)\n",
    "\n",
    "            \n",
    "            x=norm(x)\n",
    "\n",
    "            if i!=self.stages-1:\n",
    "                x=x.reshape(B,H,W,-1).permute(0,3,1,2).contiguous()\n",
    "            \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.forward_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3196, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(1,3196,64)\n",
    "lora1=torch.randn(3196,8)\n",
    "lora2=torch.randn(8,64)\n",
    "\n",
    "a1=(lora1@lora2)*a\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORA_LearnablePatchAttentionModel_classification(nn.Module):\n",
    "    def __init__(self,model,rank_of_matrix,alpha,num_classes=3):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        for i in self.model.parameters():\n",
    "            i.requires_grad=False\n",
    "        \n",
    "        self.rank_of_matrix=rank_of_matrix\n",
    "        \n",
    "        self.num_stages=self.model.stages\n",
    "\n",
    "        self.alpha=alpha\n",
    "        self.num_classes=num_classes\n",
    "\n",
    "        self.stage1_a=nn.Parameter(\n",
    "            torch.randn(rank_of_matrix,64),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.stage1_b=nn.Parameter(\n",
    "            torch.randn(3136,rank_of_matrix),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.stage2_a=nn.Parameter(\n",
    "            torch.randn(rank_of_matrix,128),requires_grad=True)\n",
    "        \n",
    "        self.stage2_b=nn.Parameter(\n",
    "            torch.randn(784,rank_of_matrix),requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.stage3_a=nn.Parameter(\n",
    "                torch.randn(rank_of_matrix,256),requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.stage3_b=nn.Parameter(\n",
    "                torch.randn(196,rank_of_matrix),requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.stage4_a=nn.Parameter(\n",
    "                torch.randn(rank_of_matrix,512),requires_grad=True\n",
    "        )\n",
    "        \n",
    "        self.stage4_b=nn.Parameter(\n",
    "                torch.randn(49,rank_of_matrix),requires_grad=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        self.mlp_head=nn.Sequential(\n",
    "            nn.Linear(512,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(128,self.num_classes)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "\n",
    "\n",
    "    def forward_once(self,x):\n",
    "\n",
    "        for i in range(self.num_stages):\n",
    "            patch_embedding=getattr(self.model,f'patch_embed{i +1}')\n",
    "            block=getattr(self.model,f'block{i +1}')\n",
    "            norm=getattr(self.model,f'norm{i +1}')\n",
    "            \n",
    "            x,H,W=patch_embedding(x)\n",
    "            x_lora=x\n",
    "\n",
    "            #forward pass of main model\n",
    "\n",
    "            for blk in block:\n",
    "                x=blk(x,H,W)\n",
    "            x=norm(x)\n",
    "            print(H,W)\n",
    "            \n",
    "            A=getattr(self,f'stage{i+1}_a')\n",
    "            B=getattr(self,f'stage{i+1}_b')\n",
    "            print(B.shape,A.shape,x_lora.shape,(B@A).shape)\n",
    "            x_lora=(B@A)*x_lora\n",
    "\n",
    "            x+=self.alpha*x_lora\n",
    "            print(x.shape)\n",
    "            print(\"lora added\")\n",
    "\n",
    "            if i!=self.num_stages-1:\n",
    "                x=rearrange(x, 'b (h w) c-> b c h w', h=H, w=W)\n",
    "                x=x.contiguous()\n",
    "\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=self.forward_once(x)\n",
    "        x=self.mlp_head(x)\n",
    "        return x\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for i in range(self.num_stages):\n",
    "            A=getattr(self,f'stage{i+1}_a')\n",
    "            B=getattr(self,f'stage{i+1}_b')\n",
    "            nn.init.kaiming_normal_(A)\n",
    "            nn.init.zeros_(B)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0019607844296842813, 0.003921568859368563, 0.005882353521883488, 0.007843137718737125, 0.009803921915590763, 0.011764707043766975, 0.013725491240620613, 0.01568627543747425, 0.01764705963432789, 0.019607843831181526, 0.021568628028035164, 0.02352941408753395, 0.02549019828438759, 0.027450982481241226, 0.029411766678094864, 0.0313725508749485, 0.03333333507180214, 0.03529411926865578, 0.037254903465509415, 0.03921568766236305, 0.04117647185921669, 0.04313725605607033, 0.045098040252923965, 0.0470588281750679, 0.04901961237192154, 0.05098038911819458, 0.05294117331504822, 0.054901961237192154, 0.05686274543404579, 0.05882352963089943, 0.06078431382775307, 0.0627450942993164, 0.06470587849617004, 0.06666666269302368, 0.06862744688987732, 0.07058823108673096, 0.0725490152835846, 0.07450979948043823, 0.07647058367729187, 0.0784313753247261, 0.08039215952157974, 0.08235294371843338, 0.08431372791528702, 0.08627451211214066, 0.0882352963089943, 0.09019608050584793, 0.09215686470270157, 0.0941176488995552, 0.09607843309640884, 0.09803921729326248, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "model=LearnablePatchAttentionModel(\n",
    "    num_heads=[1,2,5,8],\n",
    "    embeded_dimesion=[64,128,320,512],\n",
    "    depth=[3,6,40,3],\n",
    "    patch_size=7,\n",
    "    norm_layer=partial(nn.LayerNorm,eps=1e-6),\n",
    "    mlp_ratio=[8, 8, 4, 4],\n",
    "    sr_ratio=[8,4,2,1],linear=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "l=torch.load('pvt_v2_b2.pth')\n",
    "new_state_dict=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_with_keys(value):\n",
    "    value=value.split('.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.state_dict().keys():\n",
    "    if i in l.keys():\n",
    "        new_state_dict[i]=l[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True patch_embed1.proj.weight\n",
      "True patch_embed1.proj.bias\n",
      "True patch_embed1.norm.weight\n",
      "True patch_embed1.norm.bias\n",
      "True block1.0.norm1.weight\n",
      "True block1.0.norm1.bias\n",
      "False block1.0.attn.k_learnable\n",
      "False block1.0.attn.v_learnable\n",
      "True block1.0.attn.q.weight\n",
      "True block1.0.attn.q.bias\n",
      "True block1.0.attn.kv.weight\n",
      "True block1.0.attn.kv.bias\n",
      "True block1.0.attn.proj.weight\n",
      "True block1.0.attn.proj.bias\n",
      "True block1.0.attn.sr.weight\n",
      "True block1.0.attn.sr.bias\n",
      "True block1.0.attn.norm.weight\n",
      "True block1.0.attn.norm.bias\n",
      "True block1.0.norm2.weight\n",
      "True block1.0.norm2.bias\n",
      "False block1.0.conv_mlp.dwconv1.weight\n",
      "False block1.0.conv_mlp.dwconv1.bias\n",
      "False block1.0.conv_mlp.pwconv1.weight\n",
      "False block1.0.conv_mlp.pwconv1.bias\n",
      "False block1.0.conv_mlp.norm1.weight\n",
      "False block1.0.conv_mlp.norm1.bias\n",
      "False block1.0.conv_mlp.dwconv2.weight\n",
      "False block1.0.conv_mlp.dwconv2.bias\n",
      "False block1.0.conv_mlp.pwconv2.weight\n",
      "False block1.0.conv_mlp.pwconv2.bias\n",
      "False block1.0.conv_mlp.norm2.weight\n",
      "False block1.0.conv_mlp.norm2.bias\n",
      "False block1.0.conv_mlp.dwconv3.weight\n",
      "False block1.0.conv_mlp.dwconv3.bias\n",
      "False block1.0.conv_mlp.pwconv3.weight\n",
      "False block1.0.conv_mlp.pwconv3.bias\n",
      "False block1.0.conv_mlp.norm3.weight\n",
      "False block1.0.conv_mlp.norm3.bias\n",
      "False block1.0.conv_mlp.pwconv4.weight\n",
      "False block1.0.conv_mlp.pwconv4.bias\n",
      "True block1.1.norm1.weight\n",
      "True block1.1.norm1.bias\n",
      "False block1.1.attn.k_learnable\n",
      "False block1.1.attn.v_learnable\n",
      "True block1.1.attn.q.weight\n",
      "True block1.1.attn.q.bias\n",
      "True block1.1.attn.kv.weight\n",
      "True block1.1.attn.kv.bias\n",
      "True block1.1.attn.proj.weight\n",
      "True block1.1.attn.proj.bias\n",
      "True block1.1.attn.sr.weight\n",
      "True block1.1.attn.sr.bias\n",
      "True block1.1.attn.norm.weight\n",
      "True block1.1.attn.norm.bias\n",
      "True block1.1.norm2.weight\n",
      "True block1.1.norm2.bias\n",
      "False block1.1.conv_mlp.dwconv1.weight\n",
      "False block1.1.conv_mlp.dwconv1.bias\n",
      "False block1.1.conv_mlp.pwconv1.weight\n",
      "False block1.1.conv_mlp.pwconv1.bias\n",
      "False block1.1.conv_mlp.norm1.weight\n",
      "False block1.1.conv_mlp.norm1.bias\n",
      "False block1.1.conv_mlp.dwconv2.weight\n",
      "False block1.1.conv_mlp.dwconv2.bias\n",
      "False block1.1.conv_mlp.pwconv2.weight\n",
      "False block1.1.conv_mlp.pwconv2.bias\n",
      "False block1.1.conv_mlp.norm2.weight\n",
      "False block1.1.conv_mlp.norm2.bias\n",
      "False block1.1.conv_mlp.dwconv3.weight\n",
      "False block1.1.conv_mlp.dwconv3.bias\n",
      "False block1.1.conv_mlp.pwconv3.weight\n",
      "False block1.1.conv_mlp.pwconv3.bias\n",
      "False block1.1.conv_mlp.norm3.weight\n",
      "False block1.1.conv_mlp.norm3.bias\n",
      "False block1.1.conv_mlp.pwconv4.weight\n",
      "False block1.1.conv_mlp.pwconv4.bias\n",
      "True block1.2.norm1.weight\n",
      "True block1.2.norm1.bias\n",
      "False block1.2.attn.k_learnable\n",
      "False block1.2.attn.v_learnable\n",
      "True block1.2.attn.q.weight\n",
      "True block1.2.attn.q.bias\n",
      "True block1.2.attn.kv.weight\n",
      "True block1.2.attn.kv.bias\n",
      "True block1.2.attn.proj.weight\n",
      "True block1.2.attn.proj.bias\n",
      "True block1.2.attn.sr.weight\n",
      "True block1.2.attn.sr.bias\n",
      "True block1.2.attn.norm.weight\n",
      "True block1.2.attn.norm.bias\n",
      "True block1.2.norm2.weight\n",
      "True block1.2.norm2.bias\n",
      "False block1.2.conv_mlp.dwconv1.weight\n",
      "False block1.2.conv_mlp.dwconv1.bias\n",
      "False block1.2.conv_mlp.pwconv1.weight\n",
      "False block1.2.conv_mlp.pwconv1.bias\n",
      "False block1.2.conv_mlp.norm1.weight\n",
      "False block1.2.conv_mlp.norm1.bias\n",
      "False block1.2.conv_mlp.dwconv2.weight\n",
      "False block1.2.conv_mlp.dwconv2.bias\n",
      "False block1.2.conv_mlp.pwconv2.weight\n",
      "False block1.2.conv_mlp.pwconv2.bias\n",
      "False block1.2.conv_mlp.norm2.weight\n",
      "False block1.2.conv_mlp.norm2.bias\n",
      "False block1.2.conv_mlp.dwconv3.weight\n",
      "False block1.2.conv_mlp.dwconv3.bias\n",
      "False block1.2.conv_mlp.pwconv3.weight\n",
      "False block1.2.conv_mlp.pwconv3.bias\n",
      "False block1.2.conv_mlp.norm3.weight\n",
      "False block1.2.conv_mlp.norm3.bias\n",
      "False block1.2.conv_mlp.pwconv4.weight\n",
      "False block1.2.conv_mlp.pwconv4.bias\n",
      "True norm1.weight\n",
      "True norm1.bias\n",
      "True patch_embed2.proj.weight\n",
      "True patch_embed2.proj.bias\n",
      "True patch_embed2.norm.weight\n",
      "True patch_embed2.norm.bias\n",
      "True block2.0.norm1.weight\n",
      "True block2.0.norm1.bias\n",
      "False block2.0.attn.k_learnable\n",
      "False block2.0.attn.v_learnable\n",
      "True block2.0.attn.q.weight\n",
      "True block2.0.attn.q.bias\n",
      "True block2.0.attn.kv.weight\n",
      "True block2.0.attn.kv.bias\n",
      "True block2.0.attn.proj.weight\n",
      "True block2.0.attn.proj.bias\n",
      "True block2.0.attn.sr.weight\n",
      "True block2.0.attn.sr.bias\n",
      "True block2.0.attn.norm.weight\n",
      "True block2.0.attn.norm.bias\n",
      "True block2.0.norm2.weight\n",
      "True block2.0.norm2.bias\n",
      "False block2.0.conv_mlp.dwconv1.weight\n",
      "False block2.0.conv_mlp.dwconv1.bias\n",
      "False block2.0.conv_mlp.pwconv1.weight\n",
      "False block2.0.conv_mlp.pwconv1.bias\n",
      "False block2.0.conv_mlp.norm1.weight\n",
      "False block2.0.conv_mlp.norm1.bias\n",
      "False block2.0.conv_mlp.dwconv2.weight\n",
      "False block2.0.conv_mlp.dwconv2.bias\n",
      "False block2.0.conv_mlp.pwconv2.weight\n",
      "False block2.0.conv_mlp.pwconv2.bias\n",
      "False block2.0.conv_mlp.norm2.weight\n",
      "False block2.0.conv_mlp.norm2.bias\n",
      "False block2.0.conv_mlp.dwconv3.weight\n",
      "False block2.0.conv_mlp.dwconv3.bias\n",
      "False block2.0.conv_mlp.pwconv3.weight\n",
      "False block2.0.conv_mlp.pwconv3.bias\n",
      "False block2.0.conv_mlp.norm3.weight\n",
      "False block2.0.conv_mlp.norm3.bias\n",
      "False block2.0.conv_mlp.pwconv4.weight\n",
      "False block2.0.conv_mlp.pwconv4.bias\n",
      "True block2.1.norm1.weight\n",
      "True block2.1.norm1.bias\n",
      "False block2.1.attn.k_learnable\n",
      "False block2.1.attn.v_learnable\n",
      "True block2.1.attn.q.weight\n",
      "True block2.1.attn.q.bias\n",
      "True block2.1.attn.kv.weight\n",
      "True block2.1.attn.kv.bias\n",
      "True block2.1.attn.proj.weight\n",
      "True block2.1.attn.proj.bias\n",
      "True block2.1.attn.sr.weight\n",
      "True block2.1.attn.sr.bias\n",
      "True block2.1.attn.norm.weight\n",
      "True block2.1.attn.norm.bias\n",
      "True block2.1.norm2.weight\n",
      "True block2.1.norm2.bias\n",
      "False block2.1.conv_mlp.dwconv1.weight\n",
      "False block2.1.conv_mlp.dwconv1.bias\n",
      "False block2.1.conv_mlp.pwconv1.weight\n",
      "False block2.1.conv_mlp.pwconv1.bias\n",
      "False block2.1.conv_mlp.norm1.weight\n",
      "False block2.1.conv_mlp.norm1.bias\n",
      "False block2.1.conv_mlp.dwconv2.weight\n",
      "False block2.1.conv_mlp.dwconv2.bias\n",
      "False block2.1.conv_mlp.pwconv2.weight\n",
      "False block2.1.conv_mlp.pwconv2.bias\n",
      "False block2.1.conv_mlp.norm2.weight\n",
      "False block2.1.conv_mlp.norm2.bias\n",
      "False block2.1.conv_mlp.dwconv3.weight\n",
      "False block2.1.conv_mlp.dwconv3.bias\n",
      "False block2.1.conv_mlp.pwconv3.weight\n",
      "False block2.1.conv_mlp.pwconv3.bias\n",
      "False block2.1.conv_mlp.norm3.weight\n",
      "False block2.1.conv_mlp.norm3.bias\n",
      "False block2.1.conv_mlp.pwconv4.weight\n",
      "False block2.1.conv_mlp.pwconv4.bias\n",
      "True block2.2.norm1.weight\n",
      "True block2.2.norm1.bias\n",
      "False block2.2.attn.k_learnable\n",
      "False block2.2.attn.v_learnable\n",
      "True block2.2.attn.q.weight\n",
      "True block2.2.attn.q.bias\n",
      "True block2.2.attn.kv.weight\n",
      "True block2.2.attn.kv.bias\n",
      "True block2.2.attn.proj.weight\n",
      "True block2.2.attn.proj.bias\n",
      "True block2.2.attn.sr.weight\n",
      "True block2.2.attn.sr.bias\n",
      "True block2.2.attn.norm.weight\n",
      "True block2.2.attn.norm.bias\n",
      "True block2.2.norm2.weight\n",
      "True block2.2.norm2.bias\n",
      "False block2.2.conv_mlp.dwconv1.weight\n",
      "False block2.2.conv_mlp.dwconv1.bias\n",
      "False block2.2.conv_mlp.pwconv1.weight\n",
      "False block2.2.conv_mlp.pwconv1.bias\n",
      "False block2.2.conv_mlp.norm1.weight\n",
      "False block2.2.conv_mlp.norm1.bias\n",
      "False block2.2.conv_mlp.dwconv2.weight\n",
      "False block2.2.conv_mlp.dwconv2.bias\n",
      "False block2.2.conv_mlp.pwconv2.weight\n",
      "False block2.2.conv_mlp.pwconv2.bias\n",
      "False block2.2.conv_mlp.norm2.weight\n",
      "False block2.2.conv_mlp.norm2.bias\n",
      "False block2.2.conv_mlp.dwconv3.weight\n",
      "False block2.2.conv_mlp.dwconv3.bias\n",
      "False block2.2.conv_mlp.pwconv3.weight\n",
      "False block2.2.conv_mlp.pwconv3.bias\n",
      "False block2.2.conv_mlp.norm3.weight\n",
      "False block2.2.conv_mlp.norm3.bias\n",
      "False block2.2.conv_mlp.pwconv4.weight\n",
      "False block2.2.conv_mlp.pwconv4.bias\n",
      "True block2.3.norm1.weight\n",
      "True block2.3.norm1.bias\n",
      "False block2.3.attn.k_learnable\n",
      "False block2.3.attn.v_learnable\n",
      "True block2.3.attn.q.weight\n",
      "True block2.3.attn.q.bias\n",
      "True block2.3.attn.kv.weight\n",
      "True block2.3.attn.kv.bias\n",
      "True block2.3.attn.proj.weight\n",
      "True block2.3.attn.proj.bias\n",
      "True block2.3.attn.sr.weight\n",
      "True block2.3.attn.sr.bias\n",
      "True block2.3.attn.norm.weight\n",
      "True block2.3.attn.norm.bias\n",
      "True block2.3.norm2.weight\n",
      "True block2.3.norm2.bias\n",
      "False block2.3.conv_mlp.dwconv1.weight\n",
      "False block2.3.conv_mlp.dwconv1.bias\n",
      "False block2.3.conv_mlp.pwconv1.weight\n",
      "False block2.3.conv_mlp.pwconv1.bias\n",
      "False block2.3.conv_mlp.norm1.weight\n",
      "False block2.3.conv_mlp.norm1.bias\n",
      "False block2.3.conv_mlp.dwconv2.weight\n",
      "False block2.3.conv_mlp.dwconv2.bias\n",
      "False block2.3.conv_mlp.pwconv2.weight\n",
      "False block2.3.conv_mlp.pwconv2.bias\n",
      "False block2.3.conv_mlp.norm2.weight\n",
      "False block2.3.conv_mlp.norm2.bias\n",
      "False block2.3.conv_mlp.dwconv3.weight\n",
      "False block2.3.conv_mlp.dwconv3.bias\n",
      "False block2.3.conv_mlp.pwconv3.weight\n",
      "False block2.3.conv_mlp.pwconv3.bias\n",
      "False block2.3.conv_mlp.norm3.weight\n",
      "False block2.3.conv_mlp.norm3.bias\n",
      "False block2.3.conv_mlp.pwconv4.weight\n",
      "False block2.3.conv_mlp.pwconv4.bias\n",
      "False block2.4.norm1.weight\n",
      "False block2.4.norm1.bias\n",
      "False block2.4.attn.k_learnable\n",
      "False block2.4.attn.v_learnable\n",
      "False block2.4.attn.q.weight\n",
      "False block2.4.attn.q.bias\n",
      "False block2.4.attn.kv.weight\n",
      "False block2.4.attn.kv.bias\n",
      "False block2.4.attn.proj.weight\n",
      "False block2.4.attn.proj.bias\n",
      "False block2.4.attn.sr.weight\n",
      "False block2.4.attn.sr.bias\n",
      "False block2.4.attn.norm.weight\n",
      "False block2.4.attn.norm.bias\n",
      "False block2.4.norm2.weight\n",
      "False block2.4.norm2.bias\n",
      "False block2.4.conv_mlp.dwconv1.weight\n",
      "False block2.4.conv_mlp.dwconv1.bias\n",
      "False block2.4.conv_mlp.pwconv1.weight\n",
      "False block2.4.conv_mlp.pwconv1.bias\n",
      "False block2.4.conv_mlp.norm1.weight\n",
      "False block2.4.conv_mlp.norm1.bias\n",
      "False block2.4.conv_mlp.dwconv2.weight\n",
      "False block2.4.conv_mlp.dwconv2.bias\n",
      "False block2.4.conv_mlp.pwconv2.weight\n",
      "False block2.4.conv_mlp.pwconv2.bias\n",
      "False block2.4.conv_mlp.norm2.weight\n",
      "False block2.4.conv_mlp.norm2.bias\n",
      "False block2.4.conv_mlp.dwconv3.weight\n",
      "False block2.4.conv_mlp.dwconv3.bias\n",
      "False block2.4.conv_mlp.pwconv3.weight\n",
      "False block2.4.conv_mlp.pwconv3.bias\n",
      "False block2.4.conv_mlp.norm3.weight\n",
      "False block2.4.conv_mlp.norm3.bias\n",
      "False block2.4.conv_mlp.pwconv4.weight\n",
      "False block2.4.conv_mlp.pwconv4.bias\n",
      "False block2.5.norm1.weight\n",
      "False block2.5.norm1.bias\n",
      "False block2.5.attn.k_learnable\n",
      "False block2.5.attn.v_learnable\n",
      "False block2.5.attn.q.weight\n",
      "False block2.5.attn.q.bias\n",
      "False block2.5.attn.kv.weight\n",
      "False block2.5.attn.kv.bias\n",
      "False block2.5.attn.proj.weight\n",
      "False block2.5.attn.proj.bias\n",
      "False block2.5.attn.sr.weight\n",
      "False block2.5.attn.sr.bias\n",
      "False block2.5.attn.norm.weight\n",
      "False block2.5.attn.norm.bias\n",
      "False block2.5.norm2.weight\n",
      "False block2.5.norm2.bias\n",
      "False block2.5.conv_mlp.dwconv1.weight\n",
      "False block2.5.conv_mlp.dwconv1.bias\n",
      "False block2.5.conv_mlp.pwconv1.weight\n",
      "False block2.5.conv_mlp.pwconv1.bias\n",
      "False block2.5.conv_mlp.norm1.weight\n",
      "False block2.5.conv_mlp.norm1.bias\n",
      "False block2.5.conv_mlp.dwconv2.weight\n",
      "False block2.5.conv_mlp.dwconv2.bias\n",
      "False block2.5.conv_mlp.pwconv2.weight\n",
      "False block2.5.conv_mlp.pwconv2.bias\n",
      "False block2.5.conv_mlp.norm2.weight\n",
      "False block2.5.conv_mlp.norm2.bias\n",
      "False block2.5.conv_mlp.dwconv3.weight\n",
      "False block2.5.conv_mlp.dwconv3.bias\n",
      "False block2.5.conv_mlp.pwconv3.weight\n",
      "False block2.5.conv_mlp.pwconv3.bias\n",
      "False block2.5.conv_mlp.norm3.weight\n",
      "False block2.5.conv_mlp.norm3.bias\n",
      "False block2.5.conv_mlp.pwconv4.weight\n",
      "False block2.5.conv_mlp.pwconv4.bias\n",
      "True norm2.weight\n",
      "True norm2.bias\n",
      "True patch_embed3.proj.weight\n",
      "True patch_embed3.proj.bias\n",
      "True patch_embed3.norm.weight\n",
      "True patch_embed3.norm.bias\n",
      "True block3.0.norm1.weight\n",
      "True block3.0.norm1.bias\n",
      "False block3.0.attn.k_learnable\n",
      "False block3.0.attn.v_learnable\n",
      "True block3.0.attn.q.weight\n",
      "True block3.0.attn.q.bias\n",
      "True block3.0.attn.kv.weight\n",
      "True block3.0.attn.kv.bias\n",
      "True block3.0.attn.proj.weight\n",
      "True block3.0.attn.proj.bias\n",
      "True block3.0.attn.sr.weight\n",
      "True block3.0.attn.sr.bias\n",
      "True block3.0.attn.norm.weight\n",
      "True block3.0.attn.norm.bias\n",
      "True block3.0.norm2.weight\n",
      "True block3.0.norm2.bias\n",
      "False block3.0.conv_mlp.dwconv1.weight\n",
      "False block3.0.conv_mlp.dwconv1.bias\n",
      "False block3.0.conv_mlp.pwconv1.weight\n",
      "False block3.0.conv_mlp.pwconv1.bias\n",
      "False block3.0.conv_mlp.norm1.weight\n",
      "False block3.0.conv_mlp.norm1.bias\n",
      "False block3.0.conv_mlp.dwconv2.weight\n",
      "False block3.0.conv_mlp.dwconv2.bias\n",
      "False block3.0.conv_mlp.pwconv2.weight\n",
      "False block3.0.conv_mlp.pwconv2.bias\n",
      "False block3.0.conv_mlp.norm2.weight\n",
      "False block3.0.conv_mlp.norm2.bias\n",
      "False block3.0.conv_mlp.dwconv3.weight\n",
      "False block3.0.conv_mlp.dwconv3.bias\n",
      "False block3.0.conv_mlp.pwconv3.weight\n",
      "False block3.0.conv_mlp.pwconv3.bias\n",
      "False block3.0.conv_mlp.norm3.weight\n",
      "False block3.0.conv_mlp.norm3.bias\n",
      "False block3.0.conv_mlp.pwconv4.weight\n",
      "False block3.0.conv_mlp.pwconv4.bias\n",
      "True block3.1.norm1.weight\n",
      "True block3.1.norm1.bias\n",
      "False block3.1.attn.k_learnable\n",
      "False block3.1.attn.v_learnable\n",
      "True block3.1.attn.q.weight\n",
      "True block3.1.attn.q.bias\n",
      "True block3.1.attn.kv.weight\n",
      "True block3.1.attn.kv.bias\n",
      "True block3.1.attn.proj.weight\n",
      "True block3.1.attn.proj.bias\n",
      "True block3.1.attn.sr.weight\n",
      "True block3.1.attn.sr.bias\n",
      "True block3.1.attn.norm.weight\n",
      "True block3.1.attn.norm.bias\n",
      "True block3.1.norm2.weight\n",
      "True block3.1.norm2.bias\n",
      "False block3.1.conv_mlp.dwconv1.weight\n",
      "False block3.1.conv_mlp.dwconv1.bias\n",
      "False block3.1.conv_mlp.pwconv1.weight\n",
      "False block3.1.conv_mlp.pwconv1.bias\n",
      "False block3.1.conv_mlp.norm1.weight\n",
      "False block3.1.conv_mlp.norm1.bias\n",
      "False block3.1.conv_mlp.dwconv2.weight\n",
      "False block3.1.conv_mlp.dwconv2.bias\n",
      "False block3.1.conv_mlp.pwconv2.weight\n",
      "False block3.1.conv_mlp.pwconv2.bias\n",
      "False block3.1.conv_mlp.norm2.weight\n",
      "False block3.1.conv_mlp.norm2.bias\n",
      "False block3.1.conv_mlp.dwconv3.weight\n",
      "False block3.1.conv_mlp.dwconv3.bias\n",
      "False block3.1.conv_mlp.pwconv3.weight\n",
      "False block3.1.conv_mlp.pwconv3.bias\n",
      "False block3.1.conv_mlp.norm3.weight\n",
      "False block3.1.conv_mlp.norm3.bias\n",
      "False block3.1.conv_mlp.pwconv4.weight\n",
      "False block3.1.conv_mlp.pwconv4.bias\n",
      "True block3.2.norm1.weight\n",
      "True block3.2.norm1.bias\n",
      "False block3.2.attn.k_learnable\n",
      "False block3.2.attn.v_learnable\n",
      "True block3.2.attn.q.weight\n",
      "True block3.2.attn.q.bias\n",
      "True block3.2.attn.kv.weight\n",
      "True block3.2.attn.kv.bias\n",
      "True block3.2.attn.proj.weight\n",
      "True block3.2.attn.proj.bias\n",
      "True block3.2.attn.sr.weight\n",
      "True block3.2.attn.sr.bias\n",
      "True block3.2.attn.norm.weight\n",
      "True block3.2.attn.norm.bias\n",
      "True block3.2.norm2.weight\n",
      "True block3.2.norm2.bias\n",
      "False block3.2.conv_mlp.dwconv1.weight\n",
      "False block3.2.conv_mlp.dwconv1.bias\n",
      "False block3.2.conv_mlp.pwconv1.weight\n",
      "False block3.2.conv_mlp.pwconv1.bias\n",
      "False block3.2.conv_mlp.norm1.weight\n",
      "False block3.2.conv_mlp.norm1.bias\n",
      "False block3.2.conv_mlp.dwconv2.weight\n",
      "False block3.2.conv_mlp.dwconv2.bias\n",
      "False block3.2.conv_mlp.pwconv2.weight\n",
      "False block3.2.conv_mlp.pwconv2.bias\n",
      "False block3.2.conv_mlp.norm2.weight\n",
      "False block3.2.conv_mlp.norm2.bias\n",
      "False block3.2.conv_mlp.dwconv3.weight\n",
      "False block3.2.conv_mlp.dwconv3.bias\n",
      "False block3.2.conv_mlp.pwconv3.weight\n",
      "False block3.2.conv_mlp.pwconv3.bias\n",
      "False block3.2.conv_mlp.norm3.weight\n",
      "False block3.2.conv_mlp.norm3.bias\n",
      "False block3.2.conv_mlp.pwconv4.weight\n",
      "False block3.2.conv_mlp.pwconv4.bias\n",
      "True block3.3.norm1.weight\n",
      "True block3.3.norm1.bias\n",
      "False block3.3.attn.k_learnable\n",
      "False block3.3.attn.v_learnable\n",
      "True block3.3.attn.q.weight\n",
      "True block3.3.attn.q.bias\n",
      "True block3.3.attn.kv.weight\n",
      "True block3.3.attn.kv.bias\n",
      "True block3.3.attn.proj.weight\n",
      "True block3.3.attn.proj.bias\n",
      "True block3.3.attn.sr.weight\n",
      "True block3.3.attn.sr.bias\n",
      "True block3.3.attn.norm.weight\n",
      "True block3.3.attn.norm.bias\n",
      "True block3.3.norm2.weight\n",
      "True block3.3.norm2.bias\n",
      "False block3.3.conv_mlp.dwconv1.weight\n",
      "False block3.3.conv_mlp.dwconv1.bias\n",
      "False block3.3.conv_mlp.pwconv1.weight\n",
      "False block3.3.conv_mlp.pwconv1.bias\n",
      "False block3.3.conv_mlp.norm1.weight\n",
      "False block3.3.conv_mlp.norm1.bias\n",
      "False block3.3.conv_mlp.dwconv2.weight\n",
      "False block3.3.conv_mlp.dwconv2.bias\n",
      "False block3.3.conv_mlp.pwconv2.weight\n",
      "False block3.3.conv_mlp.pwconv2.bias\n",
      "False block3.3.conv_mlp.norm2.weight\n",
      "False block3.3.conv_mlp.norm2.bias\n",
      "False block3.3.conv_mlp.dwconv3.weight\n",
      "False block3.3.conv_mlp.dwconv3.bias\n",
      "False block3.3.conv_mlp.pwconv3.weight\n",
      "False block3.3.conv_mlp.pwconv3.bias\n",
      "False block3.3.conv_mlp.norm3.weight\n",
      "False block3.3.conv_mlp.norm3.bias\n",
      "False block3.3.conv_mlp.pwconv4.weight\n",
      "False block3.3.conv_mlp.pwconv4.bias\n",
      "True block3.4.norm1.weight\n",
      "True block3.4.norm1.bias\n",
      "False block3.4.attn.k_learnable\n",
      "False block3.4.attn.v_learnable\n",
      "True block3.4.attn.q.weight\n",
      "True block3.4.attn.q.bias\n",
      "True block3.4.attn.kv.weight\n",
      "True block3.4.attn.kv.bias\n",
      "True block3.4.attn.proj.weight\n",
      "True block3.4.attn.proj.bias\n",
      "True block3.4.attn.sr.weight\n",
      "True block3.4.attn.sr.bias\n",
      "True block3.4.attn.norm.weight\n",
      "True block3.4.attn.norm.bias\n",
      "True block3.4.norm2.weight\n",
      "True block3.4.norm2.bias\n",
      "False block3.4.conv_mlp.dwconv1.weight\n",
      "False block3.4.conv_mlp.dwconv1.bias\n",
      "False block3.4.conv_mlp.pwconv1.weight\n",
      "False block3.4.conv_mlp.pwconv1.bias\n",
      "False block3.4.conv_mlp.norm1.weight\n",
      "False block3.4.conv_mlp.norm1.bias\n",
      "False block3.4.conv_mlp.dwconv2.weight\n",
      "False block3.4.conv_mlp.dwconv2.bias\n",
      "False block3.4.conv_mlp.pwconv2.weight\n",
      "False block3.4.conv_mlp.pwconv2.bias\n",
      "False block3.4.conv_mlp.norm2.weight\n",
      "False block3.4.conv_mlp.norm2.bias\n",
      "False block3.4.conv_mlp.dwconv3.weight\n",
      "False block3.4.conv_mlp.dwconv3.bias\n",
      "False block3.4.conv_mlp.pwconv3.weight\n",
      "False block3.4.conv_mlp.pwconv3.bias\n",
      "False block3.4.conv_mlp.norm3.weight\n",
      "False block3.4.conv_mlp.norm3.bias\n",
      "False block3.4.conv_mlp.pwconv4.weight\n",
      "False block3.4.conv_mlp.pwconv4.bias\n",
      "True block3.5.norm1.weight\n",
      "True block3.5.norm1.bias\n",
      "False block3.5.attn.k_learnable\n",
      "False block3.5.attn.v_learnable\n",
      "True block3.5.attn.q.weight\n",
      "True block3.5.attn.q.bias\n",
      "True block3.5.attn.kv.weight\n",
      "True block3.5.attn.kv.bias\n",
      "True block3.5.attn.proj.weight\n",
      "True block3.5.attn.proj.bias\n",
      "True block3.5.attn.sr.weight\n",
      "True block3.5.attn.sr.bias\n",
      "True block3.5.attn.norm.weight\n",
      "True block3.5.attn.norm.bias\n",
      "True block3.5.norm2.weight\n",
      "True block3.5.norm2.bias\n",
      "False block3.5.conv_mlp.dwconv1.weight\n",
      "False block3.5.conv_mlp.dwconv1.bias\n",
      "False block3.5.conv_mlp.pwconv1.weight\n",
      "False block3.5.conv_mlp.pwconv1.bias\n",
      "False block3.5.conv_mlp.norm1.weight\n",
      "False block3.5.conv_mlp.norm1.bias\n",
      "False block3.5.conv_mlp.dwconv2.weight\n",
      "False block3.5.conv_mlp.dwconv2.bias\n",
      "False block3.5.conv_mlp.pwconv2.weight\n",
      "False block3.5.conv_mlp.pwconv2.bias\n",
      "False block3.5.conv_mlp.norm2.weight\n",
      "False block3.5.conv_mlp.norm2.bias\n",
      "False block3.5.conv_mlp.dwconv3.weight\n",
      "False block3.5.conv_mlp.dwconv3.bias\n",
      "False block3.5.conv_mlp.pwconv3.weight\n",
      "False block3.5.conv_mlp.pwconv3.bias\n",
      "False block3.5.conv_mlp.norm3.weight\n",
      "False block3.5.conv_mlp.norm3.bias\n",
      "False block3.5.conv_mlp.pwconv4.weight\n",
      "False block3.5.conv_mlp.pwconv4.bias\n",
      "False block3.6.norm1.weight\n",
      "False block3.6.norm1.bias\n",
      "False block3.6.attn.k_learnable\n",
      "False block3.6.attn.v_learnable\n",
      "False block3.6.attn.q.weight\n",
      "False block3.6.attn.q.bias\n",
      "False block3.6.attn.kv.weight\n",
      "False block3.6.attn.kv.bias\n",
      "False block3.6.attn.proj.weight\n",
      "False block3.6.attn.proj.bias\n",
      "False block3.6.attn.sr.weight\n",
      "False block3.6.attn.sr.bias\n",
      "False block3.6.attn.norm.weight\n",
      "False block3.6.attn.norm.bias\n",
      "False block3.6.norm2.weight\n",
      "False block3.6.norm2.bias\n",
      "False block3.6.conv_mlp.dwconv1.weight\n",
      "False block3.6.conv_mlp.dwconv1.bias\n",
      "False block3.6.conv_mlp.pwconv1.weight\n",
      "False block3.6.conv_mlp.pwconv1.bias\n",
      "False block3.6.conv_mlp.norm1.weight\n",
      "False block3.6.conv_mlp.norm1.bias\n",
      "False block3.6.conv_mlp.dwconv2.weight\n",
      "False block3.6.conv_mlp.dwconv2.bias\n",
      "False block3.6.conv_mlp.pwconv2.weight\n",
      "False block3.6.conv_mlp.pwconv2.bias\n",
      "False block3.6.conv_mlp.norm2.weight\n",
      "False block3.6.conv_mlp.norm2.bias\n",
      "False block3.6.conv_mlp.dwconv3.weight\n",
      "False block3.6.conv_mlp.dwconv3.bias\n",
      "False block3.6.conv_mlp.pwconv3.weight\n",
      "False block3.6.conv_mlp.pwconv3.bias\n",
      "False block3.6.conv_mlp.norm3.weight\n",
      "False block3.6.conv_mlp.norm3.bias\n",
      "False block3.6.conv_mlp.pwconv4.weight\n",
      "False block3.6.conv_mlp.pwconv4.bias\n",
      "False block3.7.norm1.weight\n",
      "False block3.7.norm1.bias\n",
      "False block3.7.attn.k_learnable\n",
      "False block3.7.attn.v_learnable\n",
      "False block3.7.attn.q.weight\n",
      "False block3.7.attn.q.bias\n",
      "False block3.7.attn.kv.weight\n",
      "False block3.7.attn.kv.bias\n",
      "False block3.7.attn.proj.weight\n",
      "False block3.7.attn.proj.bias\n",
      "False block3.7.attn.sr.weight\n",
      "False block3.7.attn.sr.bias\n",
      "False block3.7.attn.norm.weight\n",
      "False block3.7.attn.norm.bias\n",
      "False block3.7.norm2.weight\n",
      "False block3.7.norm2.bias\n",
      "False block3.7.conv_mlp.dwconv1.weight\n",
      "False block3.7.conv_mlp.dwconv1.bias\n",
      "False block3.7.conv_mlp.pwconv1.weight\n",
      "False block3.7.conv_mlp.pwconv1.bias\n",
      "False block3.7.conv_mlp.norm1.weight\n",
      "False block3.7.conv_mlp.norm1.bias\n",
      "False block3.7.conv_mlp.dwconv2.weight\n",
      "False block3.7.conv_mlp.dwconv2.bias\n",
      "False block3.7.conv_mlp.pwconv2.weight\n",
      "False block3.7.conv_mlp.pwconv2.bias\n",
      "False block3.7.conv_mlp.norm2.weight\n",
      "False block3.7.conv_mlp.norm2.bias\n",
      "False block3.7.conv_mlp.dwconv3.weight\n",
      "False block3.7.conv_mlp.dwconv3.bias\n",
      "False block3.7.conv_mlp.pwconv3.weight\n",
      "False block3.7.conv_mlp.pwconv3.bias\n",
      "False block3.7.conv_mlp.norm3.weight\n",
      "False block3.7.conv_mlp.norm3.bias\n",
      "False block3.7.conv_mlp.pwconv4.weight\n",
      "False block3.7.conv_mlp.pwconv4.bias\n",
      "False block3.8.norm1.weight\n",
      "False block3.8.norm1.bias\n",
      "False block3.8.attn.k_learnable\n",
      "False block3.8.attn.v_learnable\n",
      "False block3.8.attn.q.weight\n",
      "False block3.8.attn.q.bias\n",
      "False block3.8.attn.kv.weight\n",
      "False block3.8.attn.kv.bias\n",
      "False block3.8.attn.proj.weight\n",
      "False block3.8.attn.proj.bias\n",
      "False block3.8.attn.sr.weight\n",
      "False block3.8.attn.sr.bias\n",
      "False block3.8.attn.norm.weight\n",
      "False block3.8.attn.norm.bias\n",
      "False block3.8.norm2.weight\n",
      "False block3.8.norm2.bias\n",
      "False block3.8.conv_mlp.dwconv1.weight\n",
      "False block3.8.conv_mlp.dwconv1.bias\n",
      "False block3.8.conv_mlp.pwconv1.weight\n",
      "False block3.8.conv_mlp.pwconv1.bias\n",
      "False block3.8.conv_mlp.norm1.weight\n",
      "False block3.8.conv_mlp.norm1.bias\n",
      "False block3.8.conv_mlp.dwconv2.weight\n",
      "False block3.8.conv_mlp.dwconv2.bias\n",
      "False block3.8.conv_mlp.pwconv2.weight\n",
      "False block3.8.conv_mlp.pwconv2.bias\n",
      "False block3.8.conv_mlp.norm2.weight\n",
      "False block3.8.conv_mlp.norm2.bias\n",
      "False block3.8.conv_mlp.dwconv3.weight\n",
      "False block3.8.conv_mlp.dwconv3.bias\n",
      "False block3.8.conv_mlp.pwconv3.weight\n",
      "False block3.8.conv_mlp.pwconv3.bias\n",
      "False block3.8.conv_mlp.norm3.weight\n",
      "False block3.8.conv_mlp.norm3.bias\n",
      "False block3.8.conv_mlp.pwconv4.weight\n",
      "False block3.8.conv_mlp.pwconv4.bias\n",
      "False block3.9.norm1.weight\n",
      "False block3.9.norm1.bias\n",
      "False block3.9.attn.k_learnable\n",
      "False block3.9.attn.v_learnable\n",
      "False block3.9.attn.q.weight\n",
      "False block3.9.attn.q.bias\n",
      "False block3.9.attn.kv.weight\n",
      "False block3.9.attn.kv.bias\n",
      "False block3.9.attn.proj.weight\n",
      "False block3.9.attn.proj.bias\n",
      "False block3.9.attn.sr.weight\n",
      "False block3.9.attn.sr.bias\n",
      "False block3.9.attn.norm.weight\n",
      "False block3.9.attn.norm.bias\n",
      "False block3.9.norm2.weight\n",
      "False block3.9.norm2.bias\n",
      "False block3.9.conv_mlp.dwconv1.weight\n",
      "False block3.9.conv_mlp.dwconv1.bias\n",
      "False block3.9.conv_mlp.pwconv1.weight\n",
      "False block3.9.conv_mlp.pwconv1.bias\n",
      "False block3.9.conv_mlp.norm1.weight\n",
      "False block3.9.conv_mlp.norm1.bias\n",
      "False block3.9.conv_mlp.dwconv2.weight\n",
      "False block3.9.conv_mlp.dwconv2.bias\n",
      "False block3.9.conv_mlp.pwconv2.weight\n",
      "False block3.9.conv_mlp.pwconv2.bias\n",
      "False block3.9.conv_mlp.norm2.weight\n",
      "False block3.9.conv_mlp.norm2.bias\n",
      "False block3.9.conv_mlp.dwconv3.weight\n",
      "False block3.9.conv_mlp.dwconv3.bias\n",
      "False block3.9.conv_mlp.pwconv3.weight\n",
      "False block3.9.conv_mlp.pwconv3.bias\n",
      "False block3.9.conv_mlp.norm3.weight\n",
      "False block3.9.conv_mlp.norm3.bias\n",
      "False block3.9.conv_mlp.pwconv4.weight\n",
      "False block3.9.conv_mlp.pwconv4.bias\n",
      "False block3.10.norm1.weight\n",
      "False block3.10.norm1.bias\n",
      "False block3.10.attn.k_learnable\n",
      "False block3.10.attn.v_learnable\n",
      "False block3.10.attn.q.weight\n",
      "False block3.10.attn.q.bias\n",
      "False block3.10.attn.kv.weight\n",
      "False block3.10.attn.kv.bias\n",
      "False block3.10.attn.proj.weight\n",
      "False block3.10.attn.proj.bias\n",
      "False block3.10.attn.sr.weight\n",
      "False block3.10.attn.sr.bias\n",
      "False block3.10.attn.norm.weight\n",
      "False block3.10.attn.norm.bias\n",
      "False block3.10.norm2.weight\n",
      "False block3.10.norm2.bias\n",
      "False block3.10.conv_mlp.dwconv1.weight\n",
      "False block3.10.conv_mlp.dwconv1.bias\n",
      "False block3.10.conv_mlp.pwconv1.weight\n",
      "False block3.10.conv_mlp.pwconv1.bias\n",
      "False block3.10.conv_mlp.norm1.weight\n",
      "False block3.10.conv_mlp.norm1.bias\n",
      "False block3.10.conv_mlp.dwconv2.weight\n",
      "False block3.10.conv_mlp.dwconv2.bias\n",
      "False block3.10.conv_mlp.pwconv2.weight\n",
      "False block3.10.conv_mlp.pwconv2.bias\n",
      "False block3.10.conv_mlp.norm2.weight\n",
      "False block3.10.conv_mlp.norm2.bias\n",
      "False block3.10.conv_mlp.dwconv3.weight\n",
      "False block3.10.conv_mlp.dwconv3.bias\n",
      "False block3.10.conv_mlp.pwconv3.weight\n",
      "False block3.10.conv_mlp.pwconv3.bias\n",
      "False block3.10.conv_mlp.norm3.weight\n",
      "False block3.10.conv_mlp.norm3.bias\n",
      "False block3.10.conv_mlp.pwconv4.weight\n",
      "False block3.10.conv_mlp.pwconv4.bias\n",
      "False block3.11.norm1.weight\n",
      "False block3.11.norm1.bias\n",
      "False block3.11.attn.k_learnable\n",
      "False block3.11.attn.v_learnable\n",
      "False block3.11.attn.q.weight\n",
      "False block3.11.attn.q.bias\n",
      "False block3.11.attn.kv.weight\n",
      "False block3.11.attn.kv.bias\n",
      "False block3.11.attn.proj.weight\n",
      "False block3.11.attn.proj.bias\n",
      "False block3.11.attn.sr.weight\n",
      "False block3.11.attn.sr.bias\n",
      "False block3.11.attn.norm.weight\n",
      "False block3.11.attn.norm.bias\n",
      "False block3.11.norm2.weight\n",
      "False block3.11.norm2.bias\n",
      "False block3.11.conv_mlp.dwconv1.weight\n",
      "False block3.11.conv_mlp.dwconv1.bias\n",
      "False block3.11.conv_mlp.pwconv1.weight\n",
      "False block3.11.conv_mlp.pwconv1.bias\n",
      "False block3.11.conv_mlp.norm1.weight\n",
      "False block3.11.conv_mlp.norm1.bias\n",
      "False block3.11.conv_mlp.dwconv2.weight\n",
      "False block3.11.conv_mlp.dwconv2.bias\n",
      "False block3.11.conv_mlp.pwconv2.weight\n",
      "False block3.11.conv_mlp.pwconv2.bias\n",
      "False block3.11.conv_mlp.norm2.weight\n",
      "False block3.11.conv_mlp.norm2.bias\n",
      "False block3.11.conv_mlp.dwconv3.weight\n",
      "False block3.11.conv_mlp.dwconv3.bias\n",
      "False block3.11.conv_mlp.pwconv3.weight\n",
      "False block3.11.conv_mlp.pwconv3.bias\n",
      "False block3.11.conv_mlp.norm3.weight\n",
      "False block3.11.conv_mlp.norm3.bias\n",
      "False block3.11.conv_mlp.pwconv4.weight\n",
      "False block3.11.conv_mlp.pwconv4.bias\n",
      "False block3.12.norm1.weight\n",
      "False block3.12.norm1.bias\n",
      "False block3.12.attn.k_learnable\n",
      "False block3.12.attn.v_learnable\n",
      "False block3.12.attn.q.weight\n",
      "False block3.12.attn.q.bias\n",
      "False block3.12.attn.kv.weight\n",
      "False block3.12.attn.kv.bias\n",
      "False block3.12.attn.proj.weight\n",
      "False block3.12.attn.proj.bias\n",
      "False block3.12.attn.sr.weight\n",
      "False block3.12.attn.sr.bias\n",
      "False block3.12.attn.norm.weight\n",
      "False block3.12.attn.norm.bias\n",
      "False block3.12.norm2.weight\n",
      "False block3.12.norm2.bias\n",
      "False block3.12.conv_mlp.dwconv1.weight\n",
      "False block3.12.conv_mlp.dwconv1.bias\n",
      "False block3.12.conv_mlp.pwconv1.weight\n",
      "False block3.12.conv_mlp.pwconv1.bias\n",
      "False block3.12.conv_mlp.norm1.weight\n",
      "False block3.12.conv_mlp.norm1.bias\n",
      "False block3.12.conv_mlp.dwconv2.weight\n",
      "False block3.12.conv_mlp.dwconv2.bias\n",
      "False block3.12.conv_mlp.pwconv2.weight\n",
      "False block3.12.conv_mlp.pwconv2.bias\n",
      "False block3.12.conv_mlp.norm2.weight\n",
      "False block3.12.conv_mlp.norm2.bias\n",
      "False block3.12.conv_mlp.dwconv3.weight\n",
      "False block3.12.conv_mlp.dwconv3.bias\n",
      "False block3.12.conv_mlp.pwconv3.weight\n",
      "False block3.12.conv_mlp.pwconv3.bias\n",
      "False block3.12.conv_mlp.norm3.weight\n",
      "False block3.12.conv_mlp.norm3.bias\n",
      "False block3.12.conv_mlp.pwconv4.weight\n",
      "False block3.12.conv_mlp.pwconv4.bias\n",
      "False block3.13.norm1.weight\n",
      "False block3.13.norm1.bias\n",
      "False block3.13.attn.k_learnable\n",
      "False block3.13.attn.v_learnable\n",
      "False block3.13.attn.q.weight\n",
      "False block3.13.attn.q.bias\n",
      "False block3.13.attn.kv.weight\n",
      "False block3.13.attn.kv.bias\n",
      "False block3.13.attn.proj.weight\n",
      "False block3.13.attn.proj.bias\n",
      "False block3.13.attn.sr.weight\n",
      "False block3.13.attn.sr.bias\n",
      "False block3.13.attn.norm.weight\n",
      "False block3.13.attn.norm.bias\n",
      "False block3.13.norm2.weight\n",
      "False block3.13.norm2.bias\n",
      "False block3.13.conv_mlp.dwconv1.weight\n",
      "False block3.13.conv_mlp.dwconv1.bias\n",
      "False block3.13.conv_mlp.pwconv1.weight\n",
      "False block3.13.conv_mlp.pwconv1.bias\n",
      "False block3.13.conv_mlp.norm1.weight\n",
      "False block3.13.conv_mlp.norm1.bias\n",
      "False block3.13.conv_mlp.dwconv2.weight\n",
      "False block3.13.conv_mlp.dwconv2.bias\n",
      "False block3.13.conv_mlp.pwconv2.weight\n",
      "False block3.13.conv_mlp.pwconv2.bias\n",
      "False block3.13.conv_mlp.norm2.weight\n",
      "False block3.13.conv_mlp.norm2.bias\n",
      "False block3.13.conv_mlp.dwconv3.weight\n",
      "False block3.13.conv_mlp.dwconv3.bias\n",
      "False block3.13.conv_mlp.pwconv3.weight\n",
      "False block3.13.conv_mlp.pwconv3.bias\n",
      "False block3.13.conv_mlp.norm3.weight\n",
      "False block3.13.conv_mlp.norm3.bias\n",
      "False block3.13.conv_mlp.pwconv4.weight\n",
      "False block3.13.conv_mlp.pwconv4.bias\n",
      "False block3.14.norm1.weight\n",
      "False block3.14.norm1.bias\n",
      "False block3.14.attn.k_learnable\n",
      "False block3.14.attn.v_learnable\n",
      "False block3.14.attn.q.weight\n",
      "False block3.14.attn.q.bias\n",
      "False block3.14.attn.kv.weight\n",
      "False block3.14.attn.kv.bias\n",
      "False block3.14.attn.proj.weight\n",
      "False block3.14.attn.proj.bias\n",
      "False block3.14.attn.sr.weight\n",
      "False block3.14.attn.sr.bias\n",
      "False block3.14.attn.norm.weight\n",
      "False block3.14.attn.norm.bias\n",
      "False block3.14.norm2.weight\n",
      "False block3.14.norm2.bias\n",
      "False block3.14.conv_mlp.dwconv1.weight\n",
      "False block3.14.conv_mlp.dwconv1.bias\n",
      "False block3.14.conv_mlp.pwconv1.weight\n",
      "False block3.14.conv_mlp.pwconv1.bias\n",
      "False block3.14.conv_mlp.norm1.weight\n",
      "False block3.14.conv_mlp.norm1.bias\n",
      "False block3.14.conv_mlp.dwconv2.weight\n",
      "False block3.14.conv_mlp.dwconv2.bias\n",
      "False block3.14.conv_mlp.pwconv2.weight\n",
      "False block3.14.conv_mlp.pwconv2.bias\n",
      "False block3.14.conv_mlp.norm2.weight\n",
      "False block3.14.conv_mlp.norm2.bias\n",
      "False block3.14.conv_mlp.dwconv3.weight\n",
      "False block3.14.conv_mlp.dwconv3.bias\n",
      "False block3.14.conv_mlp.pwconv3.weight\n",
      "False block3.14.conv_mlp.pwconv3.bias\n",
      "False block3.14.conv_mlp.norm3.weight\n",
      "False block3.14.conv_mlp.norm3.bias\n",
      "False block3.14.conv_mlp.pwconv4.weight\n",
      "False block3.14.conv_mlp.pwconv4.bias\n",
      "False block3.15.norm1.weight\n",
      "False block3.15.norm1.bias\n",
      "False block3.15.attn.k_learnable\n",
      "False block3.15.attn.v_learnable\n",
      "False block3.15.attn.q.weight\n",
      "False block3.15.attn.q.bias\n",
      "False block3.15.attn.kv.weight\n",
      "False block3.15.attn.kv.bias\n",
      "False block3.15.attn.proj.weight\n",
      "False block3.15.attn.proj.bias\n",
      "False block3.15.attn.sr.weight\n",
      "False block3.15.attn.sr.bias\n",
      "False block3.15.attn.norm.weight\n",
      "False block3.15.attn.norm.bias\n",
      "False block3.15.norm2.weight\n",
      "False block3.15.norm2.bias\n",
      "False block3.15.conv_mlp.dwconv1.weight\n",
      "False block3.15.conv_mlp.dwconv1.bias\n",
      "False block3.15.conv_mlp.pwconv1.weight\n",
      "False block3.15.conv_mlp.pwconv1.bias\n",
      "False block3.15.conv_mlp.norm1.weight\n",
      "False block3.15.conv_mlp.norm1.bias\n",
      "False block3.15.conv_mlp.dwconv2.weight\n",
      "False block3.15.conv_mlp.dwconv2.bias\n",
      "False block3.15.conv_mlp.pwconv2.weight\n",
      "False block3.15.conv_mlp.pwconv2.bias\n",
      "False block3.15.conv_mlp.norm2.weight\n",
      "False block3.15.conv_mlp.norm2.bias\n",
      "False block3.15.conv_mlp.dwconv3.weight\n",
      "False block3.15.conv_mlp.dwconv3.bias\n",
      "False block3.15.conv_mlp.pwconv3.weight\n",
      "False block3.15.conv_mlp.pwconv3.bias\n",
      "False block3.15.conv_mlp.norm3.weight\n",
      "False block3.15.conv_mlp.norm3.bias\n",
      "False block3.15.conv_mlp.pwconv4.weight\n",
      "False block3.15.conv_mlp.pwconv4.bias\n",
      "False block3.16.norm1.weight\n",
      "False block3.16.norm1.bias\n",
      "False block3.16.attn.k_learnable\n",
      "False block3.16.attn.v_learnable\n",
      "False block3.16.attn.q.weight\n",
      "False block3.16.attn.q.bias\n",
      "False block3.16.attn.kv.weight\n",
      "False block3.16.attn.kv.bias\n",
      "False block3.16.attn.proj.weight\n",
      "False block3.16.attn.proj.bias\n",
      "False block3.16.attn.sr.weight\n",
      "False block3.16.attn.sr.bias\n",
      "False block3.16.attn.norm.weight\n",
      "False block3.16.attn.norm.bias\n",
      "False block3.16.norm2.weight\n",
      "False block3.16.norm2.bias\n",
      "False block3.16.conv_mlp.dwconv1.weight\n",
      "False block3.16.conv_mlp.dwconv1.bias\n",
      "False block3.16.conv_mlp.pwconv1.weight\n",
      "False block3.16.conv_mlp.pwconv1.bias\n",
      "False block3.16.conv_mlp.norm1.weight\n",
      "False block3.16.conv_mlp.norm1.bias\n",
      "False block3.16.conv_mlp.dwconv2.weight\n",
      "False block3.16.conv_mlp.dwconv2.bias\n",
      "False block3.16.conv_mlp.pwconv2.weight\n",
      "False block3.16.conv_mlp.pwconv2.bias\n",
      "False block3.16.conv_mlp.norm2.weight\n",
      "False block3.16.conv_mlp.norm2.bias\n",
      "False block3.16.conv_mlp.dwconv3.weight\n",
      "False block3.16.conv_mlp.dwconv3.bias\n",
      "False block3.16.conv_mlp.pwconv3.weight\n",
      "False block3.16.conv_mlp.pwconv3.bias\n",
      "False block3.16.conv_mlp.norm3.weight\n",
      "False block3.16.conv_mlp.norm3.bias\n",
      "False block3.16.conv_mlp.pwconv4.weight\n",
      "False block3.16.conv_mlp.pwconv4.bias\n",
      "False block3.17.norm1.weight\n",
      "False block3.17.norm1.bias\n",
      "False block3.17.attn.k_learnable\n",
      "False block3.17.attn.v_learnable\n",
      "False block3.17.attn.q.weight\n",
      "False block3.17.attn.q.bias\n",
      "False block3.17.attn.kv.weight\n",
      "False block3.17.attn.kv.bias\n",
      "False block3.17.attn.proj.weight\n",
      "False block3.17.attn.proj.bias\n",
      "False block3.17.attn.sr.weight\n",
      "False block3.17.attn.sr.bias\n",
      "False block3.17.attn.norm.weight\n",
      "False block3.17.attn.norm.bias\n",
      "False block3.17.norm2.weight\n",
      "False block3.17.norm2.bias\n",
      "False block3.17.conv_mlp.dwconv1.weight\n",
      "False block3.17.conv_mlp.dwconv1.bias\n",
      "False block3.17.conv_mlp.pwconv1.weight\n",
      "False block3.17.conv_mlp.pwconv1.bias\n",
      "False block3.17.conv_mlp.norm1.weight\n",
      "False block3.17.conv_mlp.norm1.bias\n",
      "False block3.17.conv_mlp.dwconv2.weight\n",
      "False block3.17.conv_mlp.dwconv2.bias\n",
      "False block3.17.conv_mlp.pwconv2.weight\n",
      "False block3.17.conv_mlp.pwconv2.bias\n",
      "False block3.17.conv_mlp.norm2.weight\n",
      "False block3.17.conv_mlp.norm2.bias\n",
      "False block3.17.conv_mlp.dwconv3.weight\n",
      "False block3.17.conv_mlp.dwconv3.bias\n",
      "False block3.17.conv_mlp.pwconv3.weight\n",
      "False block3.17.conv_mlp.pwconv3.bias\n",
      "False block3.17.conv_mlp.norm3.weight\n",
      "False block3.17.conv_mlp.norm3.bias\n",
      "False block3.17.conv_mlp.pwconv4.weight\n",
      "False block3.17.conv_mlp.pwconv4.bias\n",
      "False block3.18.norm1.weight\n",
      "False block3.18.norm1.bias\n",
      "False block3.18.attn.k_learnable\n",
      "False block3.18.attn.v_learnable\n",
      "False block3.18.attn.q.weight\n",
      "False block3.18.attn.q.bias\n",
      "False block3.18.attn.kv.weight\n",
      "False block3.18.attn.kv.bias\n",
      "False block3.18.attn.proj.weight\n",
      "False block3.18.attn.proj.bias\n",
      "False block3.18.attn.sr.weight\n",
      "False block3.18.attn.sr.bias\n",
      "False block3.18.attn.norm.weight\n",
      "False block3.18.attn.norm.bias\n",
      "False block3.18.norm2.weight\n",
      "False block3.18.norm2.bias\n",
      "False block3.18.conv_mlp.dwconv1.weight\n",
      "False block3.18.conv_mlp.dwconv1.bias\n",
      "False block3.18.conv_mlp.pwconv1.weight\n",
      "False block3.18.conv_mlp.pwconv1.bias\n",
      "False block3.18.conv_mlp.norm1.weight\n",
      "False block3.18.conv_mlp.norm1.bias\n",
      "False block3.18.conv_mlp.dwconv2.weight\n",
      "False block3.18.conv_mlp.dwconv2.bias\n",
      "False block3.18.conv_mlp.pwconv2.weight\n",
      "False block3.18.conv_mlp.pwconv2.bias\n",
      "False block3.18.conv_mlp.norm2.weight\n",
      "False block3.18.conv_mlp.norm2.bias\n",
      "False block3.18.conv_mlp.dwconv3.weight\n",
      "False block3.18.conv_mlp.dwconv3.bias\n",
      "False block3.18.conv_mlp.pwconv3.weight\n",
      "False block3.18.conv_mlp.pwconv3.bias\n",
      "False block3.18.conv_mlp.norm3.weight\n",
      "False block3.18.conv_mlp.norm3.bias\n",
      "False block3.18.conv_mlp.pwconv4.weight\n",
      "False block3.18.conv_mlp.pwconv4.bias\n",
      "False block3.19.norm1.weight\n",
      "False block3.19.norm1.bias\n",
      "False block3.19.attn.k_learnable\n",
      "False block3.19.attn.v_learnable\n",
      "False block3.19.attn.q.weight\n",
      "False block3.19.attn.q.bias\n",
      "False block3.19.attn.kv.weight\n",
      "False block3.19.attn.kv.bias\n",
      "False block3.19.attn.proj.weight\n",
      "False block3.19.attn.proj.bias\n",
      "False block3.19.attn.sr.weight\n",
      "False block3.19.attn.sr.bias\n",
      "False block3.19.attn.norm.weight\n",
      "False block3.19.attn.norm.bias\n",
      "False block3.19.norm2.weight\n",
      "False block3.19.norm2.bias\n",
      "False block3.19.conv_mlp.dwconv1.weight\n",
      "False block3.19.conv_mlp.dwconv1.bias\n",
      "False block3.19.conv_mlp.pwconv1.weight\n",
      "False block3.19.conv_mlp.pwconv1.bias\n",
      "False block3.19.conv_mlp.norm1.weight\n",
      "False block3.19.conv_mlp.norm1.bias\n",
      "False block3.19.conv_mlp.dwconv2.weight\n",
      "False block3.19.conv_mlp.dwconv2.bias\n",
      "False block3.19.conv_mlp.pwconv2.weight\n",
      "False block3.19.conv_mlp.pwconv2.bias\n",
      "False block3.19.conv_mlp.norm2.weight\n",
      "False block3.19.conv_mlp.norm2.bias\n",
      "False block3.19.conv_mlp.dwconv3.weight\n",
      "False block3.19.conv_mlp.dwconv3.bias\n",
      "False block3.19.conv_mlp.pwconv3.weight\n",
      "False block3.19.conv_mlp.pwconv3.bias\n",
      "False block3.19.conv_mlp.norm3.weight\n",
      "False block3.19.conv_mlp.norm3.bias\n",
      "False block3.19.conv_mlp.pwconv4.weight\n",
      "False block3.19.conv_mlp.pwconv4.bias\n",
      "False block3.20.norm1.weight\n",
      "False block3.20.norm1.bias\n",
      "False block3.20.attn.k_learnable\n",
      "False block3.20.attn.v_learnable\n",
      "False block3.20.attn.q.weight\n",
      "False block3.20.attn.q.bias\n",
      "False block3.20.attn.kv.weight\n",
      "False block3.20.attn.kv.bias\n",
      "False block3.20.attn.proj.weight\n",
      "False block3.20.attn.proj.bias\n",
      "False block3.20.attn.sr.weight\n",
      "False block3.20.attn.sr.bias\n",
      "False block3.20.attn.norm.weight\n",
      "False block3.20.attn.norm.bias\n",
      "False block3.20.norm2.weight\n",
      "False block3.20.norm2.bias\n",
      "False block3.20.conv_mlp.dwconv1.weight\n",
      "False block3.20.conv_mlp.dwconv1.bias\n",
      "False block3.20.conv_mlp.pwconv1.weight\n",
      "False block3.20.conv_mlp.pwconv1.bias\n",
      "False block3.20.conv_mlp.norm1.weight\n",
      "False block3.20.conv_mlp.norm1.bias\n",
      "False block3.20.conv_mlp.dwconv2.weight\n",
      "False block3.20.conv_mlp.dwconv2.bias\n",
      "False block3.20.conv_mlp.pwconv2.weight\n",
      "False block3.20.conv_mlp.pwconv2.bias\n",
      "False block3.20.conv_mlp.norm2.weight\n",
      "False block3.20.conv_mlp.norm2.bias\n",
      "False block3.20.conv_mlp.dwconv3.weight\n",
      "False block3.20.conv_mlp.dwconv3.bias\n",
      "False block3.20.conv_mlp.pwconv3.weight\n",
      "False block3.20.conv_mlp.pwconv3.bias\n",
      "False block3.20.conv_mlp.norm3.weight\n",
      "False block3.20.conv_mlp.norm3.bias\n",
      "False block3.20.conv_mlp.pwconv4.weight\n",
      "False block3.20.conv_mlp.pwconv4.bias\n",
      "False block3.21.norm1.weight\n",
      "False block3.21.norm1.bias\n",
      "False block3.21.attn.k_learnable\n",
      "False block3.21.attn.v_learnable\n",
      "False block3.21.attn.q.weight\n",
      "False block3.21.attn.q.bias\n",
      "False block3.21.attn.kv.weight\n",
      "False block3.21.attn.kv.bias\n",
      "False block3.21.attn.proj.weight\n",
      "False block3.21.attn.proj.bias\n",
      "False block3.21.attn.sr.weight\n",
      "False block3.21.attn.sr.bias\n",
      "False block3.21.attn.norm.weight\n",
      "False block3.21.attn.norm.bias\n",
      "False block3.21.norm2.weight\n",
      "False block3.21.norm2.bias\n",
      "False block3.21.conv_mlp.dwconv1.weight\n",
      "False block3.21.conv_mlp.dwconv1.bias\n",
      "False block3.21.conv_mlp.pwconv1.weight\n",
      "False block3.21.conv_mlp.pwconv1.bias\n",
      "False block3.21.conv_mlp.norm1.weight\n",
      "False block3.21.conv_mlp.norm1.bias\n",
      "False block3.21.conv_mlp.dwconv2.weight\n",
      "False block3.21.conv_mlp.dwconv2.bias\n",
      "False block3.21.conv_mlp.pwconv2.weight\n",
      "False block3.21.conv_mlp.pwconv2.bias\n",
      "False block3.21.conv_mlp.norm2.weight\n",
      "False block3.21.conv_mlp.norm2.bias\n",
      "False block3.21.conv_mlp.dwconv3.weight\n",
      "False block3.21.conv_mlp.dwconv3.bias\n",
      "False block3.21.conv_mlp.pwconv3.weight\n",
      "False block3.21.conv_mlp.pwconv3.bias\n",
      "False block3.21.conv_mlp.norm3.weight\n",
      "False block3.21.conv_mlp.norm3.bias\n",
      "False block3.21.conv_mlp.pwconv4.weight\n",
      "False block3.21.conv_mlp.pwconv4.bias\n",
      "False block3.22.norm1.weight\n",
      "False block3.22.norm1.bias\n",
      "False block3.22.attn.k_learnable\n",
      "False block3.22.attn.v_learnable\n",
      "False block3.22.attn.q.weight\n",
      "False block3.22.attn.q.bias\n",
      "False block3.22.attn.kv.weight\n",
      "False block3.22.attn.kv.bias\n",
      "False block3.22.attn.proj.weight\n",
      "False block3.22.attn.proj.bias\n",
      "False block3.22.attn.sr.weight\n",
      "False block3.22.attn.sr.bias\n",
      "False block3.22.attn.norm.weight\n",
      "False block3.22.attn.norm.bias\n",
      "False block3.22.norm2.weight\n",
      "False block3.22.norm2.bias\n",
      "False block3.22.conv_mlp.dwconv1.weight\n",
      "False block3.22.conv_mlp.dwconv1.bias\n",
      "False block3.22.conv_mlp.pwconv1.weight\n",
      "False block3.22.conv_mlp.pwconv1.bias\n",
      "False block3.22.conv_mlp.norm1.weight\n",
      "False block3.22.conv_mlp.norm1.bias\n",
      "False block3.22.conv_mlp.dwconv2.weight\n",
      "False block3.22.conv_mlp.dwconv2.bias\n",
      "False block3.22.conv_mlp.pwconv2.weight\n",
      "False block3.22.conv_mlp.pwconv2.bias\n",
      "False block3.22.conv_mlp.norm2.weight\n",
      "False block3.22.conv_mlp.norm2.bias\n",
      "False block3.22.conv_mlp.dwconv3.weight\n",
      "False block3.22.conv_mlp.dwconv3.bias\n",
      "False block3.22.conv_mlp.pwconv3.weight\n",
      "False block3.22.conv_mlp.pwconv3.bias\n",
      "False block3.22.conv_mlp.norm3.weight\n",
      "False block3.22.conv_mlp.norm3.bias\n",
      "False block3.22.conv_mlp.pwconv4.weight\n",
      "False block3.22.conv_mlp.pwconv4.bias\n",
      "False block3.23.norm1.weight\n",
      "False block3.23.norm1.bias\n",
      "False block3.23.attn.k_learnable\n",
      "False block3.23.attn.v_learnable\n",
      "False block3.23.attn.q.weight\n",
      "False block3.23.attn.q.bias\n",
      "False block3.23.attn.kv.weight\n",
      "False block3.23.attn.kv.bias\n",
      "False block3.23.attn.proj.weight\n",
      "False block3.23.attn.proj.bias\n",
      "False block3.23.attn.sr.weight\n",
      "False block3.23.attn.sr.bias\n",
      "False block3.23.attn.norm.weight\n",
      "False block3.23.attn.norm.bias\n",
      "False block3.23.norm2.weight\n",
      "False block3.23.norm2.bias\n",
      "False block3.23.conv_mlp.dwconv1.weight\n",
      "False block3.23.conv_mlp.dwconv1.bias\n",
      "False block3.23.conv_mlp.pwconv1.weight\n",
      "False block3.23.conv_mlp.pwconv1.bias\n",
      "False block3.23.conv_mlp.norm1.weight\n",
      "False block3.23.conv_mlp.norm1.bias\n",
      "False block3.23.conv_mlp.dwconv2.weight\n",
      "False block3.23.conv_mlp.dwconv2.bias\n",
      "False block3.23.conv_mlp.pwconv2.weight\n",
      "False block3.23.conv_mlp.pwconv2.bias\n",
      "False block3.23.conv_mlp.norm2.weight\n",
      "False block3.23.conv_mlp.norm2.bias\n",
      "False block3.23.conv_mlp.dwconv3.weight\n",
      "False block3.23.conv_mlp.dwconv3.bias\n",
      "False block3.23.conv_mlp.pwconv3.weight\n",
      "False block3.23.conv_mlp.pwconv3.bias\n",
      "False block3.23.conv_mlp.norm3.weight\n",
      "False block3.23.conv_mlp.norm3.bias\n",
      "False block3.23.conv_mlp.pwconv4.weight\n",
      "False block3.23.conv_mlp.pwconv4.bias\n",
      "False block3.24.norm1.weight\n",
      "False block3.24.norm1.bias\n",
      "False block3.24.attn.k_learnable\n",
      "False block3.24.attn.v_learnable\n",
      "False block3.24.attn.q.weight\n",
      "False block3.24.attn.q.bias\n",
      "False block3.24.attn.kv.weight\n",
      "False block3.24.attn.kv.bias\n",
      "False block3.24.attn.proj.weight\n",
      "False block3.24.attn.proj.bias\n",
      "False block3.24.attn.sr.weight\n",
      "False block3.24.attn.sr.bias\n",
      "False block3.24.attn.norm.weight\n",
      "False block3.24.attn.norm.bias\n",
      "False block3.24.norm2.weight\n",
      "False block3.24.norm2.bias\n",
      "False block3.24.conv_mlp.dwconv1.weight\n",
      "False block3.24.conv_mlp.dwconv1.bias\n",
      "False block3.24.conv_mlp.pwconv1.weight\n",
      "False block3.24.conv_mlp.pwconv1.bias\n",
      "False block3.24.conv_mlp.norm1.weight\n",
      "False block3.24.conv_mlp.norm1.bias\n",
      "False block3.24.conv_mlp.dwconv2.weight\n",
      "False block3.24.conv_mlp.dwconv2.bias\n",
      "False block3.24.conv_mlp.pwconv2.weight\n",
      "False block3.24.conv_mlp.pwconv2.bias\n",
      "False block3.24.conv_mlp.norm2.weight\n",
      "False block3.24.conv_mlp.norm2.bias\n",
      "False block3.24.conv_mlp.dwconv3.weight\n",
      "False block3.24.conv_mlp.dwconv3.bias\n",
      "False block3.24.conv_mlp.pwconv3.weight\n",
      "False block3.24.conv_mlp.pwconv3.bias\n",
      "False block3.24.conv_mlp.norm3.weight\n",
      "False block3.24.conv_mlp.norm3.bias\n",
      "False block3.24.conv_mlp.pwconv4.weight\n",
      "False block3.24.conv_mlp.pwconv4.bias\n",
      "False block3.25.norm1.weight\n",
      "False block3.25.norm1.bias\n",
      "False block3.25.attn.k_learnable\n",
      "False block3.25.attn.v_learnable\n",
      "False block3.25.attn.q.weight\n",
      "False block3.25.attn.q.bias\n",
      "False block3.25.attn.kv.weight\n",
      "False block3.25.attn.kv.bias\n",
      "False block3.25.attn.proj.weight\n",
      "False block3.25.attn.proj.bias\n",
      "False block3.25.attn.sr.weight\n",
      "False block3.25.attn.sr.bias\n",
      "False block3.25.attn.norm.weight\n",
      "False block3.25.attn.norm.bias\n",
      "False block3.25.norm2.weight\n",
      "False block3.25.norm2.bias\n",
      "False block3.25.conv_mlp.dwconv1.weight\n",
      "False block3.25.conv_mlp.dwconv1.bias\n",
      "False block3.25.conv_mlp.pwconv1.weight\n",
      "False block3.25.conv_mlp.pwconv1.bias\n",
      "False block3.25.conv_mlp.norm1.weight\n",
      "False block3.25.conv_mlp.norm1.bias\n",
      "False block3.25.conv_mlp.dwconv2.weight\n",
      "False block3.25.conv_mlp.dwconv2.bias\n",
      "False block3.25.conv_mlp.pwconv2.weight\n",
      "False block3.25.conv_mlp.pwconv2.bias\n",
      "False block3.25.conv_mlp.norm2.weight\n",
      "False block3.25.conv_mlp.norm2.bias\n",
      "False block3.25.conv_mlp.dwconv3.weight\n",
      "False block3.25.conv_mlp.dwconv3.bias\n",
      "False block3.25.conv_mlp.pwconv3.weight\n",
      "False block3.25.conv_mlp.pwconv3.bias\n",
      "False block3.25.conv_mlp.norm3.weight\n",
      "False block3.25.conv_mlp.norm3.bias\n",
      "False block3.25.conv_mlp.pwconv4.weight\n",
      "False block3.25.conv_mlp.pwconv4.bias\n",
      "False block3.26.norm1.weight\n",
      "False block3.26.norm1.bias\n",
      "False block3.26.attn.k_learnable\n",
      "False block3.26.attn.v_learnable\n",
      "False block3.26.attn.q.weight\n",
      "False block3.26.attn.q.bias\n",
      "False block3.26.attn.kv.weight\n",
      "False block3.26.attn.kv.bias\n",
      "False block3.26.attn.proj.weight\n",
      "False block3.26.attn.proj.bias\n",
      "False block3.26.attn.sr.weight\n",
      "False block3.26.attn.sr.bias\n",
      "False block3.26.attn.norm.weight\n",
      "False block3.26.attn.norm.bias\n",
      "False block3.26.norm2.weight\n",
      "False block3.26.norm2.bias\n",
      "False block3.26.conv_mlp.dwconv1.weight\n",
      "False block3.26.conv_mlp.dwconv1.bias\n",
      "False block3.26.conv_mlp.pwconv1.weight\n",
      "False block3.26.conv_mlp.pwconv1.bias\n",
      "False block3.26.conv_mlp.norm1.weight\n",
      "False block3.26.conv_mlp.norm1.bias\n",
      "False block3.26.conv_mlp.dwconv2.weight\n",
      "False block3.26.conv_mlp.dwconv2.bias\n",
      "False block3.26.conv_mlp.pwconv2.weight\n",
      "False block3.26.conv_mlp.pwconv2.bias\n",
      "False block3.26.conv_mlp.norm2.weight\n",
      "False block3.26.conv_mlp.norm2.bias\n",
      "False block3.26.conv_mlp.dwconv3.weight\n",
      "False block3.26.conv_mlp.dwconv3.bias\n",
      "False block3.26.conv_mlp.pwconv3.weight\n",
      "False block3.26.conv_mlp.pwconv3.bias\n",
      "False block3.26.conv_mlp.norm3.weight\n",
      "False block3.26.conv_mlp.norm3.bias\n",
      "False block3.26.conv_mlp.pwconv4.weight\n",
      "False block3.26.conv_mlp.pwconv4.bias\n",
      "False block3.27.norm1.weight\n",
      "False block3.27.norm1.bias\n",
      "False block3.27.attn.k_learnable\n",
      "False block3.27.attn.v_learnable\n",
      "False block3.27.attn.q.weight\n",
      "False block3.27.attn.q.bias\n",
      "False block3.27.attn.kv.weight\n",
      "False block3.27.attn.kv.bias\n",
      "False block3.27.attn.proj.weight\n",
      "False block3.27.attn.proj.bias\n",
      "False block3.27.attn.sr.weight\n",
      "False block3.27.attn.sr.bias\n",
      "False block3.27.attn.norm.weight\n",
      "False block3.27.attn.norm.bias\n",
      "False block3.27.norm2.weight\n",
      "False block3.27.norm2.bias\n",
      "False block3.27.conv_mlp.dwconv1.weight\n",
      "False block3.27.conv_mlp.dwconv1.bias\n",
      "False block3.27.conv_mlp.pwconv1.weight\n",
      "False block3.27.conv_mlp.pwconv1.bias\n",
      "False block3.27.conv_mlp.norm1.weight\n",
      "False block3.27.conv_mlp.norm1.bias\n",
      "False block3.27.conv_mlp.dwconv2.weight\n",
      "False block3.27.conv_mlp.dwconv2.bias\n",
      "False block3.27.conv_mlp.pwconv2.weight\n",
      "False block3.27.conv_mlp.pwconv2.bias\n",
      "False block3.27.conv_mlp.norm2.weight\n",
      "False block3.27.conv_mlp.norm2.bias\n",
      "False block3.27.conv_mlp.dwconv3.weight\n",
      "False block3.27.conv_mlp.dwconv3.bias\n",
      "False block3.27.conv_mlp.pwconv3.weight\n",
      "False block3.27.conv_mlp.pwconv3.bias\n",
      "False block3.27.conv_mlp.norm3.weight\n",
      "False block3.27.conv_mlp.norm3.bias\n",
      "False block3.27.conv_mlp.pwconv4.weight\n",
      "False block3.27.conv_mlp.pwconv4.bias\n",
      "False block3.28.norm1.weight\n",
      "False block3.28.norm1.bias\n",
      "False block3.28.attn.k_learnable\n",
      "False block3.28.attn.v_learnable\n",
      "False block3.28.attn.q.weight\n",
      "False block3.28.attn.q.bias\n",
      "False block3.28.attn.kv.weight\n",
      "False block3.28.attn.kv.bias\n",
      "False block3.28.attn.proj.weight\n",
      "False block3.28.attn.proj.bias\n",
      "False block3.28.attn.sr.weight\n",
      "False block3.28.attn.sr.bias\n",
      "False block3.28.attn.norm.weight\n",
      "False block3.28.attn.norm.bias\n",
      "False block3.28.norm2.weight\n",
      "False block3.28.norm2.bias\n",
      "False block3.28.conv_mlp.dwconv1.weight\n",
      "False block3.28.conv_mlp.dwconv1.bias\n",
      "False block3.28.conv_mlp.pwconv1.weight\n",
      "False block3.28.conv_mlp.pwconv1.bias\n",
      "False block3.28.conv_mlp.norm1.weight\n",
      "False block3.28.conv_mlp.norm1.bias\n",
      "False block3.28.conv_mlp.dwconv2.weight\n",
      "False block3.28.conv_mlp.dwconv2.bias\n",
      "False block3.28.conv_mlp.pwconv2.weight\n",
      "False block3.28.conv_mlp.pwconv2.bias\n",
      "False block3.28.conv_mlp.norm2.weight\n",
      "False block3.28.conv_mlp.norm2.bias\n",
      "False block3.28.conv_mlp.dwconv3.weight\n",
      "False block3.28.conv_mlp.dwconv3.bias\n",
      "False block3.28.conv_mlp.pwconv3.weight\n",
      "False block3.28.conv_mlp.pwconv3.bias\n",
      "False block3.28.conv_mlp.norm3.weight\n",
      "False block3.28.conv_mlp.norm3.bias\n",
      "False block3.28.conv_mlp.pwconv4.weight\n",
      "False block3.28.conv_mlp.pwconv4.bias\n",
      "False block3.29.norm1.weight\n",
      "False block3.29.norm1.bias\n",
      "False block3.29.attn.k_learnable\n",
      "False block3.29.attn.v_learnable\n",
      "False block3.29.attn.q.weight\n",
      "False block3.29.attn.q.bias\n",
      "False block3.29.attn.kv.weight\n",
      "False block3.29.attn.kv.bias\n",
      "False block3.29.attn.proj.weight\n",
      "False block3.29.attn.proj.bias\n",
      "False block3.29.attn.sr.weight\n",
      "False block3.29.attn.sr.bias\n",
      "False block3.29.attn.norm.weight\n",
      "False block3.29.attn.norm.bias\n",
      "False block3.29.norm2.weight\n",
      "False block3.29.norm2.bias\n",
      "False block3.29.conv_mlp.dwconv1.weight\n",
      "False block3.29.conv_mlp.dwconv1.bias\n",
      "False block3.29.conv_mlp.pwconv1.weight\n",
      "False block3.29.conv_mlp.pwconv1.bias\n",
      "False block3.29.conv_mlp.norm1.weight\n",
      "False block3.29.conv_mlp.norm1.bias\n",
      "False block3.29.conv_mlp.dwconv2.weight\n",
      "False block3.29.conv_mlp.dwconv2.bias\n",
      "False block3.29.conv_mlp.pwconv2.weight\n",
      "False block3.29.conv_mlp.pwconv2.bias\n",
      "False block3.29.conv_mlp.norm2.weight\n",
      "False block3.29.conv_mlp.norm2.bias\n",
      "False block3.29.conv_mlp.dwconv3.weight\n",
      "False block3.29.conv_mlp.dwconv3.bias\n",
      "False block3.29.conv_mlp.pwconv3.weight\n",
      "False block3.29.conv_mlp.pwconv3.bias\n",
      "False block3.29.conv_mlp.norm3.weight\n",
      "False block3.29.conv_mlp.norm3.bias\n",
      "False block3.29.conv_mlp.pwconv4.weight\n",
      "False block3.29.conv_mlp.pwconv4.bias\n",
      "False block3.30.norm1.weight\n",
      "False block3.30.norm1.bias\n",
      "False block3.30.attn.k_learnable\n",
      "False block3.30.attn.v_learnable\n",
      "False block3.30.attn.q.weight\n",
      "False block3.30.attn.q.bias\n",
      "False block3.30.attn.kv.weight\n",
      "False block3.30.attn.kv.bias\n",
      "False block3.30.attn.proj.weight\n",
      "False block3.30.attn.proj.bias\n",
      "False block3.30.attn.sr.weight\n",
      "False block3.30.attn.sr.bias\n",
      "False block3.30.attn.norm.weight\n",
      "False block3.30.attn.norm.bias\n",
      "False block3.30.norm2.weight\n",
      "False block3.30.norm2.bias\n",
      "False block3.30.conv_mlp.dwconv1.weight\n",
      "False block3.30.conv_mlp.dwconv1.bias\n",
      "False block3.30.conv_mlp.pwconv1.weight\n",
      "False block3.30.conv_mlp.pwconv1.bias\n",
      "False block3.30.conv_mlp.norm1.weight\n",
      "False block3.30.conv_mlp.norm1.bias\n",
      "False block3.30.conv_mlp.dwconv2.weight\n",
      "False block3.30.conv_mlp.dwconv2.bias\n",
      "False block3.30.conv_mlp.pwconv2.weight\n",
      "False block3.30.conv_mlp.pwconv2.bias\n",
      "False block3.30.conv_mlp.norm2.weight\n",
      "False block3.30.conv_mlp.norm2.bias\n",
      "False block3.30.conv_mlp.dwconv3.weight\n",
      "False block3.30.conv_mlp.dwconv3.bias\n",
      "False block3.30.conv_mlp.pwconv3.weight\n",
      "False block3.30.conv_mlp.pwconv3.bias\n",
      "False block3.30.conv_mlp.norm3.weight\n",
      "False block3.30.conv_mlp.norm3.bias\n",
      "False block3.30.conv_mlp.pwconv4.weight\n",
      "False block3.30.conv_mlp.pwconv4.bias\n",
      "False block3.31.norm1.weight\n",
      "False block3.31.norm1.bias\n",
      "False block3.31.attn.k_learnable\n",
      "False block3.31.attn.v_learnable\n",
      "False block3.31.attn.q.weight\n",
      "False block3.31.attn.q.bias\n",
      "False block3.31.attn.kv.weight\n",
      "False block3.31.attn.kv.bias\n",
      "False block3.31.attn.proj.weight\n",
      "False block3.31.attn.proj.bias\n",
      "False block3.31.attn.sr.weight\n",
      "False block3.31.attn.sr.bias\n",
      "False block3.31.attn.norm.weight\n",
      "False block3.31.attn.norm.bias\n",
      "False block3.31.norm2.weight\n",
      "False block3.31.norm2.bias\n",
      "False block3.31.conv_mlp.dwconv1.weight\n",
      "False block3.31.conv_mlp.dwconv1.bias\n",
      "False block3.31.conv_mlp.pwconv1.weight\n",
      "False block3.31.conv_mlp.pwconv1.bias\n",
      "False block3.31.conv_mlp.norm1.weight\n",
      "False block3.31.conv_mlp.norm1.bias\n",
      "False block3.31.conv_mlp.dwconv2.weight\n",
      "False block3.31.conv_mlp.dwconv2.bias\n",
      "False block3.31.conv_mlp.pwconv2.weight\n",
      "False block3.31.conv_mlp.pwconv2.bias\n",
      "False block3.31.conv_mlp.norm2.weight\n",
      "False block3.31.conv_mlp.norm2.bias\n",
      "False block3.31.conv_mlp.dwconv3.weight\n",
      "False block3.31.conv_mlp.dwconv3.bias\n",
      "False block3.31.conv_mlp.pwconv3.weight\n",
      "False block3.31.conv_mlp.pwconv3.bias\n",
      "False block3.31.conv_mlp.norm3.weight\n",
      "False block3.31.conv_mlp.norm3.bias\n",
      "False block3.31.conv_mlp.pwconv4.weight\n",
      "False block3.31.conv_mlp.pwconv4.bias\n",
      "False block3.32.norm1.weight\n",
      "False block3.32.norm1.bias\n",
      "False block3.32.attn.k_learnable\n",
      "False block3.32.attn.v_learnable\n",
      "False block3.32.attn.q.weight\n",
      "False block3.32.attn.q.bias\n",
      "False block3.32.attn.kv.weight\n",
      "False block3.32.attn.kv.bias\n",
      "False block3.32.attn.proj.weight\n",
      "False block3.32.attn.proj.bias\n",
      "False block3.32.attn.sr.weight\n",
      "False block3.32.attn.sr.bias\n",
      "False block3.32.attn.norm.weight\n",
      "False block3.32.attn.norm.bias\n",
      "False block3.32.norm2.weight\n",
      "False block3.32.norm2.bias\n",
      "False block3.32.conv_mlp.dwconv1.weight\n",
      "False block3.32.conv_mlp.dwconv1.bias\n",
      "False block3.32.conv_mlp.pwconv1.weight\n",
      "False block3.32.conv_mlp.pwconv1.bias\n",
      "False block3.32.conv_mlp.norm1.weight\n",
      "False block3.32.conv_mlp.norm1.bias\n",
      "False block3.32.conv_mlp.dwconv2.weight\n",
      "False block3.32.conv_mlp.dwconv2.bias\n",
      "False block3.32.conv_mlp.pwconv2.weight\n",
      "False block3.32.conv_mlp.pwconv2.bias\n",
      "False block3.32.conv_mlp.norm2.weight\n",
      "False block3.32.conv_mlp.norm2.bias\n",
      "False block3.32.conv_mlp.dwconv3.weight\n",
      "False block3.32.conv_mlp.dwconv3.bias\n",
      "False block3.32.conv_mlp.pwconv3.weight\n",
      "False block3.32.conv_mlp.pwconv3.bias\n",
      "False block3.32.conv_mlp.norm3.weight\n",
      "False block3.32.conv_mlp.norm3.bias\n",
      "False block3.32.conv_mlp.pwconv4.weight\n",
      "False block3.32.conv_mlp.pwconv4.bias\n",
      "False block3.33.norm1.weight\n",
      "False block3.33.norm1.bias\n",
      "False block3.33.attn.k_learnable\n",
      "False block3.33.attn.v_learnable\n",
      "False block3.33.attn.q.weight\n",
      "False block3.33.attn.q.bias\n",
      "False block3.33.attn.kv.weight\n",
      "False block3.33.attn.kv.bias\n",
      "False block3.33.attn.proj.weight\n",
      "False block3.33.attn.proj.bias\n",
      "False block3.33.attn.sr.weight\n",
      "False block3.33.attn.sr.bias\n",
      "False block3.33.attn.norm.weight\n",
      "False block3.33.attn.norm.bias\n",
      "False block3.33.norm2.weight\n",
      "False block3.33.norm2.bias\n",
      "False block3.33.conv_mlp.dwconv1.weight\n",
      "False block3.33.conv_mlp.dwconv1.bias\n",
      "False block3.33.conv_mlp.pwconv1.weight\n",
      "False block3.33.conv_mlp.pwconv1.bias\n",
      "False block3.33.conv_mlp.norm1.weight\n",
      "False block3.33.conv_mlp.norm1.bias\n",
      "False block3.33.conv_mlp.dwconv2.weight\n",
      "False block3.33.conv_mlp.dwconv2.bias\n",
      "False block3.33.conv_mlp.pwconv2.weight\n",
      "False block3.33.conv_mlp.pwconv2.bias\n",
      "False block3.33.conv_mlp.norm2.weight\n",
      "False block3.33.conv_mlp.norm2.bias\n",
      "False block3.33.conv_mlp.dwconv3.weight\n",
      "False block3.33.conv_mlp.dwconv3.bias\n",
      "False block3.33.conv_mlp.pwconv3.weight\n",
      "False block3.33.conv_mlp.pwconv3.bias\n",
      "False block3.33.conv_mlp.norm3.weight\n",
      "False block3.33.conv_mlp.norm3.bias\n",
      "False block3.33.conv_mlp.pwconv4.weight\n",
      "False block3.33.conv_mlp.pwconv4.bias\n",
      "False block3.34.norm1.weight\n",
      "False block3.34.norm1.bias\n",
      "False block3.34.attn.k_learnable\n",
      "False block3.34.attn.v_learnable\n",
      "False block3.34.attn.q.weight\n",
      "False block3.34.attn.q.bias\n",
      "False block3.34.attn.kv.weight\n",
      "False block3.34.attn.kv.bias\n",
      "False block3.34.attn.proj.weight\n",
      "False block3.34.attn.proj.bias\n",
      "False block3.34.attn.sr.weight\n",
      "False block3.34.attn.sr.bias\n",
      "False block3.34.attn.norm.weight\n",
      "False block3.34.attn.norm.bias\n",
      "False block3.34.norm2.weight\n",
      "False block3.34.norm2.bias\n",
      "False block3.34.conv_mlp.dwconv1.weight\n",
      "False block3.34.conv_mlp.dwconv1.bias\n",
      "False block3.34.conv_mlp.pwconv1.weight\n",
      "False block3.34.conv_mlp.pwconv1.bias\n",
      "False block3.34.conv_mlp.norm1.weight\n",
      "False block3.34.conv_mlp.norm1.bias\n",
      "False block3.34.conv_mlp.dwconv2.weight\n",
      "False block3.34.conv_mlp.dwconv2.bias\n",
      "False block3.34.conv_mlp.pwconv2.weight\n",
      "False block3.34.conv_mlp.pwconv2.bias\n",
      "False block3.34.conv_mlp.norm2.weight\n",
      "False block3.34.conv_mlp.norm2.bias\n",
      "False block3.34.conv_mlp.dwconv3.weight\n",
      "False block3.34.conv_mlp.dwconv3.bias\n",
      "False block3.34.conv_mlp.pwconv3.weight\n",
      "False block3.34.conv_mlp.pwconv3.bias\n",
      "False block3.34.conv_mlp.norm3.weight\n",
      "False block3.34.conv_mlp.norm3.bias\n",
      "False block3.34.conv_mlp.pwconv4.weight\n",
      "False block3.34.conv_mlp.pwconv4.bias\n",
      "False block3.35.norm1.weight\n",
      "False block3.35.norm1.bias\n",
      "False block3.35.attn.k_learnable\n",
      "False block3.35.attn.v_learnable\n",
      "False block3.35.attn.q.weight\n",
      "False block3.35.attn.q.bias\n",
      "False block3.35.attn.kv.weight\n",
      "False block3.35.attn.kv.bias\n",
      "False block3.35.attn.proj.weight\n",
      "False block3.35.attn.proj.bias\n",
      "False block3.35.attn.sr.weight\n",
      "False block3.35.attn.sr.bias\n",
      "False block3.35.attn.norm.weight\n",
      "False block3.35.attn.norm.bias\n",
      "False block3.35.norm2.weight\n",
      "False block3.35.norm2.bias\n",
      "False block3.35.conv_mlp.dwconv1.weight\n",
      "False block3.35.conv_mlp.dwconv1.bias\n",
      "False block3.35.conv_mlp.pwconv1.weight\n",
      "False block3.35.conv_mlp.pwconv1.bias\n",
      "False block3.35.conv_mlp.norm1.weight\n",
      "False block3.35.conv_mlp.norm1.bias\n",
      "False block3.35.conv_mlp.dwconv2.weight\n",
      "False block3.35.conv_mlp.dwconv2.bias\n",
      "False block3.35.conv_mlp.pwconv2.weight\n",
      "False block3.35.conv_mlp.pwconv2.bias\n",
      "False block3.35.conv_mlp.norm2.weight\n",
      "False block3.35.conv_mlp.norm2.bias\n",
      "False block3.35.conv_mlp.dwconv3.weight\n",
      "False block3.35.conv_mlp.dwconv3.bias\n",
      "False block3.35.conv_mlp.pwconv3.weight\n",
      "False block3.35.conv_mlp.pwconv3.bias\n",
      "False block3.35.conv_mlp.norm3.weight\n",
      "False block3.35.conv_mlp.norm3.bias\n",
      "False block3.35.conv_mlp.pwconv4.weight\n",
      "False block3.35.conv_mlp.pwconv4.bias\n",
      "False block3.36.norm1.weight\n",
      "False block3.36.norm1.bias\n",
      "False block3.36.attn.k_learnable\n",
      "False block3.36.attn.v_learnable\n",
      "False block3.36.attn.q.weight\n",
      "False block3.36.attn.q.bias\n",
      "False block3.36.attn.kv.weight\n",
      "False block3.36.attn.kv.bias\n",
      "False block3.36.attn.proj.weight\n",
      "False block3.36.attn.proj.bias\n",
      "False block3.36.attn.sr.weight\n",
      "False block3.36.attn.sr.bias\n",
      "False block3.36.attn.norm.weight\n",
      "False block3.36.attn.norm.bias\n",
      "False block3.36.norm2.weight\n",
      "False block3.36.norm2.bias\n",
      "False block3.36.conv_mlp.dwconv1.weight\n",
      "False block3.36.conv_mlp.dwconv1.bias\n",
      "False block3.36.conv_mlp.pwconv1.weight\n",
      "False block3.36.conv_mlp.pwconv1.bias\n",
      "False block3.36.conv_mlp.norm1.weight\n",
      "False block3.36.conv_mlp.norm1.bias\n",
      "False block3.36.conv_mlp.dwconv2.weight\n",
      "False block3.36.conv_mlp.dwconv2.bias\n",
      "False block3.36.conv_mlp.pwconv2.weight\n",
      "False block3.36.conv_mlp.pwconv2.bias\n",
      "False block3.36.conv_mlp.norm2.weight\n",
      "False block3.36.conv_mlp.norm2.bias\n",
      "False block3.36.conv_mlp.dwconv3.weight\n",
      "False block3.36.conv_mlp.dwconv3.bias\n",
      "False block3.36.conv_mlp.pwconv3.weight\n",
      "False block3.36.conv_mlp.pwconv3.bias\n",
      "False block3.36.conv_mlp.norm3.weight\n",
      "False block3.36.conv_mlp.norm3.bias\n",
      "False block3.36.conv_mlp.pwconv4.weight\n",
      "False block3.36.conv_mlp.pwconv4.bias\n",
      "False block3.37.norm1.weight\n",
      "False block3.37.norm1.bias\n",
      "False block3.37.attn.k_learnable\n",
      "False block3.37.attn.v_learnable\n",
      "False block3.37.attn.q.weight\n",
      "False block3.37.attn.q.bias\n",
      "False block3.37.attn.kv.weight\n",
      "False block3.37.attn.kv.bias\n",
      "False block3.37.attn.proj.weight\n",
      "False block3.37.attn.proj.bias\n",
      "False block3.37.attn.sr.weight\n",
      "False block3.37.attn.sr.bias\n",
      "False block3.37.attn.norm.weight\n",
      "False block3.37.attn.norm.bias\n",
      "False block3.37.norm2.weight\n",
      "False block3.37.norm2.bias\n",
      "False block3.37.conv_mlp.dwconv1.weight\n",
      "False block3.37.conv_mlp.dwconv1.bias\n",
      "False block3.37.conv_mlp.pwconv1.weight\n",
      "False block3.37.conv_mlp.pwconv1.bias\n",
      "False block3.37.conv_mlp.norm1.weight\n",
      "False block3.37.conv_mlp.norm1.bias\n",
      "False block3.37.conv_mlp.dwconv2.weight\n",
      "False block3.37.conv_mlp.dwconv2.bias\n",
      "False block3.37.conv_mlp.pwconv2.weight\n",
      "False block3.37.conv_mlp.pwconv2.bias\n",
      "False block3.37.conv_mlp.norm2.weight\n",
      "False block3.37.conv_mlp.norm2.bias\n",
      "False block3.37.conv_mlp.dwconv3.weight\n",
      "False block3.37.conv_mlp.dwconv3.bias\n",
      "False block3.37.conv_mlp.pwconv3.weight\n",
      "False block3.37.conv_mlp.pwconv3.bias\n",
      "False block3.37.conv_mlp.norm3.weight\n",
      "False block3.37.conv_mlp.norm3.bias\n",
      "False block3.37.conv_mlp.pwconv4.weight\n",
      "False block3.37.conv_mlp.pwconv4.bias\n",
      "False block3.38.norm1.weight\n",
      "False block3.38.norm1.bias\n",
      "False block3.38.attn.k_learnable\n",
      "False block3.38.attn.v_learnable\n",
      "False block3.38.attn.q.weight\n",
      "False block3.38.attn.q.bias\n",
      "False block3.38.attn.kv.weight\n",
      "False block3.38.attn.kv.bias\n",
      "False block3.38.attn.proj.weight\n",
      "False block3.38.attn.proj.bias\n",
      "False block3.38.attn.sr.weight\n",
      "False block3.38.attn.sr.bias\n",
      "False block3.38.attn.norm.weight\n",
      "False block3.38.attn.norm.bias\n",
      "False block3.38.norm2.weight\n",
      "False block3.38.norm2.bias\n",
      "False block3.38.conv_mlp.dwconv1.weight\n",
      "False block3.38.conv_mlp.dwconv1.bias\n",
      "False block3.38.conv_mlp.pwconv1.weight\n",
      "False block3.38.conv_mlp.pwconv1.bias\n",
      "False block3.38.conv_mlp.norm1.weight\n",
      "False block3.38.conv_mlp.norm1.bias\n",
      "False block3.38.conv_mlp.dwconv2.weight\n",
      "False block3.38.conv_mlp.dwconv2.bias\n",
      "False block3.38.conv_mlp.pwconv2.weight\n",
      "False block3.38.conv_mlp.pwconv2.bias\n",
      "False block3.38.conv_mlp.norm2.weight\n",
      "False block3.38.conv_mlp.norm2.bias\n",
      "False block3.38.conv_mlp.dwconv3.weight\n",
      "False block3.38.conv_mlp.dwconv3.bias\n",
      "False block3.38.conv_mlp.pwconv3.weight\n",
      "False block3.38.conv_mlp.pwconv3.bias\n",
      "False block3.38.conv_mlp.norm3.weight\n",
      "False block3.38.conv_mlp.norm3.bias\n",
      "False block3.38.conv_mlp.pwconv4.weight\n",
      "False block3.38.conv_mlp.pwconv4.bias\n",
      "False block3.39.norm1.weight\n",
      "False block3.39.norm1.bias\n",
      "False block3.39.attn.k_learnable\n",
      "False block3.39.attn.v_learnable\n",
      "False block3.39.attn.q.weight\n",
      "False block3.39.attn.q.bias\n",
      "False block3.39.attn.kv.weight\n",
      "False block3.39.attn.kv.bias\n",
      "False block3.39.attn.proj.weight\n",
      "False block3.39.attn.proj.bias\n",
      "False block3.39.attn.sr.weight\n",
      "False block3.39.attn.sr.bias\n",
      "False block3.39.attn.norm.weight\n",
      "False block3.39.attn.norm.bias\n",
      "False block3.39.norm2.weight\n",
      "False block3.39.norm2.bias\n",
      "False block3.39.conv_mlp.dwconv1.weight\n",
      "False block3.39.conv_mlp.dwconv1.bias\n",
      "False block3.39.conv_mlp.pwconv1.weight\n",
      "False block3.39.conv_mlp.pwconv1.bias\n",
      "False block3.39.conv_mlp.norm1.weight\n",
      "False block3.39.conv_mlp.norm1.bias\n",
      "False block3.39.conv_mlp.dwconv2.weight\n",
      "False block3.39.conv_mlp.dwconv2.bias\n",
      "False block3.39.conv_mlp.pwconv2.weight\n",
      "False block3.39.conv_mlp.pwconv2.bias\n",
      "False block3.39.conv_mlp.norm2.weight\n",
      "False block3.39.conv_mlp.norm2.bias\n",
      "False block3.39.conv_mlp.dwconv3.weight\n",
      "False block3.39.conv_mlp.dwconv3.bias\n",
      "False block3.39.conv_mlp.pwconv3.weight\n",
      "False block3.39.conv_mlp.pwconv3.bias\n",
      "False block3.39.conv_mlp.norm3.weight\n",
      "False block3.39.conv_mlp.norm3.bias\n",
      "False block3.39.conv_mlp.pwconv4.weight\n",
      "False block3.39.conv_mlp.pwconv4.bias\n",
      "True norm3.weight\n",
      "True norm3.bias\n",
      "True patch_embed4.proj.weight\n",
      "True patch_embed4.proj.bias\n",
      "True patch_embed4.norm.weight\n",
      "True patch_embed4.norm.bias\n",
      "True block4.0.norm1.weight\n",
      "True block4.0.norm1.bias\n",
      "False block4.0.attn.k_learnable\n",
      "False block4.0.attn.v_learnable\n",
      "True block4.0.attn.q.weight\n",
      "True block4.0.attn.q.bias\n",
      "True block4.0.attn.kv.weight\n",
      "True block4.0.attn.kv.bias\n",
      "True block4.0.attn.proj.weight\n",
      "True block4.0.attn.proj.bias\n",
      "True block4.0.norm2.weight\n",
      "True block4.0.norm2.bias\n",
      "False block4.0.conv_mlp.dwconv1.weight\n",
      "False block4.0.conv_mlp.dwconv1.bias\n",
      "False block4.0.conv_mlp.pwconv1.weight\n",
      "False block4.0.conv_mlp.pwconv1.bias\n",
      "False block4.0.conv_mlp.norm1.weight\n",
      "False block4.0.conv_mlp.norm1.bias\n",
      "False block4.0.conv_mlp.dwconv2.weight\n",
      "False block4.0.conv_mlp.dwconv2.bias\n",
      "False block4.0.conv_mlp.pwconv2.weight\n",
      "False block4.0.conv_mlp.pwconv2.bias\n",
      "False block4.0.conv_mlp.norm2.weight\n",
      "False block4.0.conv_mlp.norm2.bias\n",
      "False block4.0.conv_mlp.dwconv3.weight\n",
      "False block4.0.conv_mlp.dwconv3.bias\n",
      "False block4.0.conv_mlp.pwconv3.weight\n",
      "False block4.0.conv_mlp.pwconv3.bias\n",
      "False block4.0.conv_mlp.norm3.weight\n",
      "False block4.0.conv_mlp.norm3.bias\n",
      "False block4.0.conv_mlp.pwconv4.weight\n",
      "False block4.0.conv_mlp.pwconv4.bias\n",
      "True block4.1.norm1.weight\n",
      "True block4.1.norm1.bias\n",
      "False block4.1.attn.k_learnable\n",
      "False block4.1.attn.v_learnable\n",
      "True block4.1.attn.q.weight\n",
      "True block4.1.attn.q.bias\n",
      "True block4.1.attn.kv.weight\n",
      "True block4.1.attn.kv.bias\n",
      "True block4.1.attn.proj.weight\n",
      "True block4.1.attn.proj.bias\n",
      "True block4.1.norm2.weight\n",
      "True block4.1.norm2.bias\n",
      "False block4.1.conv_mlp.dwconv1.weight\n",
      "False block4.1.conv_mlp.dwconv1.bias\n",
      "False block4.1.conv_mlp.pwconv1.weight\n",
      "False block4.1.conv_mlp.pwconv1.bias\n",
      "False block4.1.conv_mlp.norm1.weight\n",
      "False block4.1.conv_mlp.norm1.bias\n",
      "False block4.1.conv_mlp.dwconv2.weight\n",
      "False block4.1.conv_mlp.dwconv2.bias\n",
      "False block4.1.conv_mlp.pwconv2.weight\n",
      "False block4.1.conv_mlp.pwconv2.bias\n",
      "False block4.1.conv_mlp.norm2.weight\n",
      "False block4.1.conv_mlp.norm2.bias\n",
      "False block4.1.conv_mlp.dwconv3.weight\n",
      "False block4.1.conv_mlp.dwconv3.bias\n",
      "False block4.1.conv_mlp.pwconv3.weight\n",
      "False block4.1.conv_mlp.pwconv3.bias\n",
      "False block4.1.conv_mlp.norm3.weight\n",
      "False block4.1.conv_mlp.norm3.bias\n",
      "False block4.1.conv_mlp.pwconv4.weight\n",
      "False block4.1.conv_mlp.pwconv4.bias\n",
      "True block4.2.norm1.weight\n",
      "True block4.2.norm1.bias\n",
      "False block4.2.attn.k_learnable\n",
      "False block4.2.attn.v_learnable\n",
      "True block4.2.attn.q.weight\n",
      "True block4.2.attn.q.bias\n",
      "True block4.2.attn.kv.weight\n",
      "True block4.2.attn.kv.bias\n",
      "True block4.2.attn.proj.weight\n",
      "True block4.2.attn.proj.bias\n",
      "True block4.2.norm2.weight\n",
      "True block4.2.norm2.bias\n",
      "False block4.2.conv_mlp.dwconv1.weight\n",
      "False block4.2.conv_mlp.dwconv1.bias\n",
      "False block4.2.conv_mlp.pwconv1.weight\n",
      "False block4.2.conv_mlp.pwconv1.bias\n",
      "False block4.2.conv_mlp.norm1.weight\n",
      "False block4.2.conv_mlp.norm1.bias\n",
      "False block4.2.conv_mlp.dwconv2.weight\n",
      "False block4.2.conv_mlp.dwconv2.bias\n",
      "False block4.2.conv_mlp.pwconv2.weight\n",
      "False block4.2.conv_mlp.pwconv2.bias\n",
      "False block4.2.conv_mlp.norm2.weight\n",
      "False block4.2.conv_mlp.norm2.bias\n",
      "False block4.2.conv_mlp.dwconv3.weight\n",
      "False block4.2.conv_mlp.dwconv3.bias\n",
      "False block4.2.conv_mlp.pwconv3.weight\n",
      "False block4.2.conv_mlp.pwconv3.bias\n",
      "False block4.2.conv_mlp.norm3.weight\n",
      "False block4.2.conv_mlp.norm3.bias\n",
      "False block4.2.conv_mlp.pwconv4.weight\n",
      "False block4.2.conv_mlp.pwconv4.bias\n",
      "True norm4.weight\n",
      "True norm4.bias\n"
     ]
    }
   ],
   "source": [
    "for named,parameters in model.named_parameters():\n",
    "    print(named in l.keys(),named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict,\"ovtparameters.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l.keys():\n",
    "    if i in model.state_dict().keys():\n",
    "        if False in l[i]==model.state_dict()[i]:\n",
    "            print( False in l[i]==model.state_dict()[i],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

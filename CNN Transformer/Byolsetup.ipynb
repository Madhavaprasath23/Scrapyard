{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import torchmetrics\n",
    "import datetime\n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _cfg\n",
    "from ConvFFN import CONVFFN\n",
    "from OwnPVTpt2 import LearnablePatchAttentionModel\n",
    "import albumentations as A\n",
    "import torch.distributed as dist\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, def_val):\n",
    "    return def_val if val is None else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaybeSyncBatchnorm(is_distributed = None):\n",
    "    is_distributed = default(is_distributed, dist.is_initialized() and dist.get_world_size() > 1)\n",
    "    return nn.SyncBatchNorm if is_distributed else nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Byol_loss_fn(z, detached_grad):\n",
    "    z_norm = F.normalize(z, dim=-1, p=2)\n",
    "    detached_grad_norm = F.normalize(detached_grad, dim=-1, p=2)\n",
    "\n",
    "    return 2 - 2 * (z_norm*detached_grad_norm).sum(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.011111111380159855, 0.02222222276031971, 0.03333333507180214, 0.04444444552063942, 0.0555555559694767, 0.06666666269302368, 0.07777778059244156, 0.08888889104127884, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "model = LearnablePatchAttentionModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 56, 56]) Projection\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 3136, 64]) Stagesout_0\n",
      "torch.Size([1, 128, 28, 28]) Projection\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 784, 128]) Stagesout_1\n",
      "torch.Size([1, 256, 14, 14]) Projection\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 196, 256]) Stagesout_2\n",
      "torch.Size([1, 512, 7, 7]) Projection\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 49, 512]) Stagesout_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 3, 224, 224)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Linear(512, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.randn(1, 49, 512).mean(dim=1)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dim,hidden_dim,out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim,hidden_dim)\n",
    "        self.batchnorm1 = MaybeSyncBatchnorm()(hidden_dim)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(.2)\n",
    "        self.batchnorm2 = MaybeSyncBatchnorm()(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,out_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m,nn.Linear):\n",
    "            trunc_normal_(m.weight,std=.02)\n",
    "            if isinstance(m,nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias,0)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Boyol(nn.Module):\n",
    "    def __init__(self, modela, modelb, projection_layer_size=1024,projection_hidden_layer_size=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        assert modela!=None and modelb!=None, \"Both models must be defined\"\n",
    "        assert modela.embeded_dimesion[-1] == modelb.embeded_dimesion[-1], \"Embedding dimensions of both models must be equal\"\n",
    "        \n",
    "        self.modela = modela\n",
    "        self.modelb = modelb\n",
    "        for i in self.modelb.parameters():\n",
    "            i.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "        self.projection_layera = MLP(self.modela.embeded_dimesion[-1],projection_hidden_layer_size,projection_layer_size)\n",
    "        self.projection_layerb = MLP(self.modelb.embeded_dimesion[-1],projection_hidden_layer_size,projection_layer_size)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z1 = self.modela(x)\n",
    "\n",
    "\n",
    "        z1 = self.projection_layera(z1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z2 = self.modelb(y)\n",
    "            z2 = self.projection_layerb(z2)\n",
    "\n",
    "        return z1, z2 \n",
    "\n",
    "    def update_average(self, tau, online_parameter, offline_parameter):\n",
    "        return tau*offline_parameter+(1-tau)*online_parameter\n",
    "\n",
    "    def update_model_b(self):\n",
    "\n",
    "        for online_parameters, offline_parameters in zip(self.modela.parameters(), self.modelb.parameters()):\n",
    "            a=online_parameters.data\n",
    "            offline_parameters.data = self.update_average(\n",
    "                0.99, online_parameters.data, offline_parameters.data)\n",
    "            b=offline_parameters.data\n",
    "            print(\"Update from \",a,\" to \",b)\n",
    "        print(\"Model B Updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.011111111380159855, 0.02222222276031971, 0.03333333507180214, 0.04444444552063942, 0.0555555559694767, 0.06666666269302368, 0.07777778059244156, 0.08888889104127884, 0.10000000149011612]\n",
      "[0.0, 0.011111111380159855, 0.02222222276031971, 0.03333333507180214, 0.04444444552063942, 0.0555555559694767, 0.06666666269302368, 0.07777778059244156, 0.08888889104127884, 0.10000000149011612]\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "y = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "model_boyol = Model_Boyol(LearnablePatchAttentionModel(),\n",
    "                          LearnablePatchAttentionModel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 56, 56]) Projection\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 3136, 64]) Stagesout_0\n",
      "torch.Size([1, 128, 28, 28]) Projection\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 784, 128]) Stagesout_1\n",
      "torch.Size([1, 256, 14, 14]) Projection\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 196, 256]) Stagesout_2\n",
      "torch.Size([1, 512, 7, 7]) Projection\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 49, 512]) Stagesout_3\n",
      "torch.Size([1, 64, 56, 56]) Projection\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 3136, 64]) Stagesout_0\n",
      "torch.Size([1, 128, 28, 28]) Projection\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 784, 128]) Stagesout_1\n",
      "torch.Size([1, 256, 14, 14]) Projection\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 196, 256]) Stagesout_2\n",
      "torch.Size([1, 512, 7, 7]) Projection\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 49, 512]) Stagesout_3\n",
      "torch.Size([1, 64, 56, 56]) Projection\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 3136, 64]) Stagesout_0\n",
      "torch.Size([1, 128, 28, 28]) Projection\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 784, 128]) Stagesout_1\n",
      "torch.Size([1, 256, 14, 14]) Projection\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 196, 256]) Stagesout_2\n",
      "torch.Size([1, 512, 7, 7]) Projection\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 49, 512]) Stagesout_3\n",
      "torch.Size([1, 64, 56, 56]) Projection\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\n",
      "torch.Size([1, 3136, 64]) Stagesout_0\n",
      "torch.Size([1, 128, 28, 28]) Projection\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 28, 28]) torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 784, 128]) Stagesout_1\n",
      "torch.Size([1, 256, 14, 14]) Projection\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 256, 14, 14]) torch.Size([1, 256, 14, 14])\n",
      "torch.Size([1, 196, 256]) Stagesout_2\n",
      "torch.Size([1, 512, 7, 7]) Projection\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7]) torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 49, 512]) Stagesout_3\n"
     ]
    }
   ],
   "source": [
    "z1,z2=model_boyol(x, y)\n",
    "z3,z4=model_boyol(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(train_loader,optimizer,model,criterion,device,epoch,log_interval=100,transform=None):\n",
    "    assert transform!=None, \"Transform must be defined\"\n",
    "    running_loss=0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, image in tqdm(enumerate(train_loader),total=len(enumerate(train_loader))):\n",
    "\n",
    "        image=image.to(device)\n",
    "        transformed_image = transform(image=image.detach().numpy())\n",
    "        transformed_image = transformed_image['image']\n",
    "        transformed_image = transformed_image.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z1,z2=model(image,transformed_image)\n",
    "        loss1 = criterion(z1,z2.detach())\n",
    "\n",
    "        z1,z2=model(transformed_image,image)\n",
    "        loss2 = criterion(z1,z2.detach())\n",
    "\n",
    "        loss = loss1+loss2\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        model.update_model_b()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.8f}'.format(\n",
    "                epoch, batch_idx * len(image), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()))\n",
    "        running_loss+=loss.item()\n",
    "    return running_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

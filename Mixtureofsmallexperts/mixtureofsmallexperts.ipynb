{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange,repeat\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lambda_ffn(nn.Module):\n",
    "    def __init__(self,input_size,hidden_dim,out_size):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.out_size=out_size\n",
    "\n",
    "        self.layer1=nn.Linear(self.input_size,self.hidden_dim)\n",
    "        self.layer2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.layer3=nn.Linear(self.hidden_dim,self.out_size)\n",
    "\n",
    "        self.gelu=nn.GELU()\n",
    "        self.lamda=nn.Parameter(torch.normal(mean=0.0,std=1.0,size=(1,1)))\n",
    "        self.lamda.data.clamp_(-1,1)\n",
    "        self.x2=0\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        nn.init.kaiming_normal_(self.layer1.weight,\n",
    "                                mode='fan_out',\n",
    "                                nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.layer2.weight,\n",
    "                                mode='fan_out',\n",
    "                                nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.layer2.weight,\n",
    "                                mode='fan_out',\n",
    "                                nonlinearity='leaky_relu')\n",
    "    def forward(self,x):\n",
    "        x=self.gelu(self.layer1(x))\n",
    "        x=self.gelu(self.layer2(x))\n",
    "        x=self.gelu(self.layer3(x))\n",
    "\n",
    "        x = self.lamda * x\n",
    "        self.x2=x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self,input_dim,num_heads,qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.dim=input_dim\n",
    "        self.num_heads=num_heads\n",
    "        \n",
    "        self.qkv_bias=qkv_bias\n",
    "\n",
    "        self.qkv_scale=self.dim\n",
    "\n",
    "        self.norm=nn.LayerNorm(self.dim)\n",
    "\n",
    "        self.qkv=nn.Linear(self.input_dim,self.dim*3,bias=self.qkv_bias)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        qkv=self.qkv(x)\n",
    "        values=rearrange(qkv,'b n (h d k)-> k b h n d',k=3,h=self.num_heads)\n",
    "        \n",
    "        q,k,v=tuple(values)\n",
    "\n",
    "        attn=torch.einsum('b h i d,b h j d -> b h i j',q,k)\n",
    "        \n",
    "        attn = attn * (self.qkv_scale**0.5)\n",
    "\n",
    "        attn = torch.einsum('b h i j, b h j d -> b h i d',attn,v)\n",
    "\n",
    "        return attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiple_lamdaffn(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dimension,n_experts):\n",
    "        super().__init__()\n",
    "        self.ffn_experts=nn.ModuleList([])\n",
    "        self.normlayer=nn.LayerNorm(input_dim)\n",
    "        self.n_experts=n_experts\n",
    "        self.w_ffn= nn.Parameter(torch.randn(self.n_experts,1),requires_grad=True)\n",
    "        for i in range(n_experts):\n",
    "            self.ffn_experts.append(\n",
    "                lambda_ffn(input_size=input_dim,hidden_dim=hidden_dimension,out_size=input_dim)\n",
    "            )\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()  \n",
    "    def forward(self,x):\n",
    "        x1=x\n",
    "        output=0\n",
    "        for i in range(self.n_experts):\n",
    "            if i==0:\n",
    "                output=rearrange(self.ffn_experts[i](x),pattern='b n d -> () b n d')\n",
    "            else:\n",
    "                output=torch.cat([output,rearrange(self.ffn_experts[i](x),pattern='b n d -> () b n d')],dim=0)\n",
    "        output=torch.einsum('e b n d, e o->b n d',output,self.w_ffn)\n",
    "        return x1 + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patch_embedding(nn.Module):\n",
    "    def __init__(self,patch_size,inchannels,padding,out_dim,stride=None):\n",
    "        super().__init__()\n",
    "\n",
    "        print(patch_size,stride,padding)\n",
    "        self.conv=nn.Conv2d(\n",
    "                in_channels=inchannels,\n",
    "                out_channels=out_dim,\n",
    "                stride=stride,\n",
    "                kernel_size=patch_size,\n",
    "                padding=padding\n",
    "        )\n",
    "        self.layernorm=nn.LayerNorm(out_dim)\n",
    "        self.gelu=nn.GELU()\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        b,c,h,w= x.shape\n",
    "        x=self.gelu(x)\n",
    "        x=rearrange(x,'b c h w -> b (h w) c')\n",
    "        x=self.layernorm(x)\n",
    "        return x ,b,c,h,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,image_size,input_dim,total_stages,num_heads,qkv_bias,mlp_ratio,n_experts,patch_size,padding,stride):\n",
    "        super().__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.total_stages=total_stages\n",
    "        self.qkv_bias=qkv_bias\n",
    "        self.mlp_ratio=mlp_ratio\n",
    "        self.n_experts=list(map(int,n_experts))\n",
    "\n",
    "        self.total_blocks=nn.ModuleList([])\n",
    "        self.patch_embedding=nn.ModuleList([])\n",
    "\n",
    "        self.padding=padding\n",
    "        self.stride=stride\n",
    "        self.patch_size=patch_size\n",
    "        for i in range(self.total_stages):\n",
    "            mlp_hidden=int(self.input_dim[i]*self.mlp_ratio[i])\n",
    "            if i==0:\n",
    "                out_dim=self.input_dim[i]\n",
    "                in_dim=3\n",
    "                self.height=(image_size-self.patch_size[i]+ 2*self.padding[i])//self.stride[i]\n",
    "                self.height+=1\n",
    "            else:\n",
    "                in_dim=self.input_dim[i-1]\n",
    "                out_dim=self.input_dim[i]\n",
    "                self.height = (self.height-self.patch_size[i]+ 2*self.padding[i])//self.stride[i]\n",
    "                self.height+=1\n",
    "\n",
    "\n",
    "            \n",
    "            current_block=nn.ModuleList([\n",
    "                patch_embedding(patch_size=self.patch_size[i],inchannels=in_dim,out_dim=out_dim,\n",
    "                                padding=self.padding[i],stride=self.stride[i]),\n",
    "\n",
    "                MultiHeadedAttention(input_dim=self.input_dim[i],\n",
    "                                     num_heads=self.num_heads[i],qkv_bias=self.qkv_bias),\n",
    "                Multiple_lamdaffn(input_dim=int(self.input_dim[i]),\n",
    "                                  hidden_dimension=mlp_hidden,\n",
    "                                  n_experts=self.n_experts[i])\n",
    "            ])\n",
    "            self.total_blocks.append(\n",
    "                current_block\n",
    "            )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        i=0\n",
    "        for patch,attn,mlp in self.total_blocks:\n",
    "            print(x.shape)\n",
    "            x,b,c,h,w=patch(x)\n",
    "            x_attn=attn(x)\n",
    "            x_attn=rearrange(x_attn,'b h n c -> b n (c h)')\n",
    "            x_mlp=mlp(x_attn)\n",
    "            x=x_mlp\n",
    "            x=rearrange(x,'b (h w) d -> b d h w',h=h,w=w)\n",
    "            i+=1\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 2 1\n",
      "3 2 0\n",
      "2 1 0\n"
     ]
    }
   ],
   "source": [
    "model=Block(\n",
    "    input_dim=[16,32,64],\n",
    "    total_stages=3,\n",
    "    num_heads=[2,4,8],\n",
    "    qkv_bias=True,\n",
    "    mlp_ratio=[2.0,3.0,3.0],\n",
    "    n_experts=[3.0,5.0,6.0],\n",
    "    patch_size=[5,3,2],\n",
    "    stride=[2,2,1],\n",
    "    padding=[1,0,0],\n",
    "    image_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 16, 15, 15])\n",
      "torch.Size([1, 32, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 6, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,32,32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[i.numel() for i in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486908"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=nn.Sequential(\n",
    "    nn.Linear(4096*3,4096),\n",
    "    nn.Linear(4096,4096),\n",
    "    nn.Linear(4096,100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=[i.numel() for i in model2.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67526756"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.flatten.Flatten"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=nn.Sequential(\n",
    "    nn.Conv2d(3,16,5),\n",
    "    nn.Conv2d(16,32,3),\n",
    "    nn.Conv2d(32,32,1),\n",
    "    nn.Flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6912"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param=[i.numel() for i in model3.parameters()]\n",
    "sum(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(\n",
    "    x=[450_000,522014,67526756],\n",
    "    y=['CNN','Transformer','MLP']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007730476494383944"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "522014/67526756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
